{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtQMSOf9OzZx"
   },
   "source": [
    "# Faster R-CNN Implementations by Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플 이미지 다운 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIu5NahaO_ee",
    "outputId": "c66ed78d-9d10-4e8a-ffc3-0e11e22e0cac"
   },
   "source": [
    "github file download URL format   \n",
    "https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 폴더에 zebras.jpg가 없으면 실행\n",
    "!wget -nc https://raw.githubusercontent.com/imguru-mooc/AI_VISION/main/3_day/3.Faster-RCNN/zebras.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDYZ7vi1OzZ-"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtfCn2JLOzZ_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkYeGFzROzZ_",
    "outputId": "f55cfb6d-5f3c-4ed5-a5eb-2ab69d473301"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(DEVICE, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsS56MswOzaA"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzYtfVrvOzaA"
   },
   "source": [
    "### 1) Visualize image and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "9bxrr3PhOzaB",
    "outputId": "2e0b3a31-b287-4fe1-b605-441a16d8f042"
   },
   "outputs": [],
   "source": [
    "# In this example, only use 1 image, i.e, batch_size=1\n",
    "# input image could be of any size\n",
    "\n",
    "img0 = cv2.imread(\"zebras.jpg\")\n",
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "print(img0.shape)\n",
    "plt.imshow(img0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw5XJRcYOzaB"
   },
   "outputs": [],
   "source": [
    "# object information : a set of bounding boxes [x1, y1, x2, y2] \n",
    "# and their labels\n",
    "bbox0 = np.array([[223, 782, 623, 1074], [597, 695, 1038, 1050], \n",
    "                  [1088, 699, 1452, 1057], [1544, 771, 1914, 1063]]) \n",
    "labels = np.array([1, 1, 1, 1]) # 0: background, 1: zebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "JhVp-EDaOzaC",
    "outputId": "d813f3b7-7fd0-454f-d393-bac788aadbad"
   },
   "outputs": [],
   "source": [
    "# display bounding box and labels\n",
    "\n",
    "img0_clone = np.copy(img0)\n",
    "for i in range(len(bbox0)):\n",
    "    cv2.rectangle(img0_clone, (bbox0[i][0], bbox0[i][1]), \n",
    "                              (bbox0[i][2], bbox0[i][3]),\n",
    "                 color=(0, 255, 0), thickness=10)\n",
    "plt.imshow(img0_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEsXbZh6OzaC"
   },
   "source": [
    "### 2) Resize image and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "RMZ05NEIOzaC",
    "outputId": "2e477a51-b71c-4d6f-d8fe-8dbe54da2e9c"
   },
   "outputs": [],
   "source": [
    "# resize the input images to h=800, w=800\n",
    "\n",
    "img = cv2.resize(img0, dsize=(800, 800), interpolation=cv2.INTER_CUBIC)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(img)\n",
    "# plt.grid(True, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "klO8M1deOzaD",
    "outputId": "7719f5ec-9fba-448b-9954-a17e5fd7c232"
   },
   "outputs": [],
   "source": [
    "# change the bounding box coordinates\n",
    "# original image size : (1333, 2000)\n",
    "\n",
    "Wratio = 800/img0.shape[1]\n",
    "Hratio = 800/img0.shape[0]\n",
    "\n",
    "print(Wratio, Hratio)\n",
    "\n",
    "ratioList = [Wratio, Hratio, Wratio, Hratio]\n",
    "bbox = []\n",
    "\n",
    "for box in bbox0:\n",
    "    box = [int(a*b) for a, b in zip(box, ratioList)]\n",
    "    bbox.append(box)\n",
    "    \n",
    "bbox = np.array(bbox)\n",
    "print(bbox)\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), color=(0, 255, 0), thickness=5)\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_YHXvGeOzaD"
   },
   "source": [
    "## Define Feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdDtKxYoOzaD"
   },
   "source": [
    "### 1) Load pretrained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "referenced_widgets": [
      "4ec3d0bb6d244fbbb8f79d7a8035bdf5",
      "daa3e6f30b6149f4a44c78e8ba143981",
      "8a8b1558c7a94aacbc6cb4053b5b57a9",
      "86eb0a8fc8644e8bb513143b5399302a",
      "e0597676159b4ef7a6596a2b933a3418",
      "b4fe9a8476ed4390b3c4a6a73fd1ca9b",
      "828fcccc6d18425d904ac4e8d639e74b",
      "28c0d215912440fa99b12023a66a26a0"
     ]
    },
    "id": "O3xafelMOzaE",
    "outputId": "1bb074da-ed7d-46cd-a379-812e6f7073ef"
   },
   "outputs": [],
   "source": [
    "# only print feature extraction part of VGG16\n",
    "\n",
    "# model = torchvision.models.vgg16(weights='VGG16_Weights.DEFAULT').to(DEVICE)\n",
    "model = torchvision.models.vgg16(pretrained=True).to(DEVICE)\n",
    "features = list(model.features)\n",
    "print(len(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPrXQtv7OzaE"
   },
   "source": [
    "### 2) Only collect required layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCDtARVjOzaE",
    "outputId": "899b921f-d4ae-49ca-d773-1e73dbb89d44"
   },
   "outputs": [],
   "source": [
    "# only collect layers with output feature map size (W, H) < 50\n",
    "\n",
    "dummy_img = torch.zeros((1, 3, 800, 800)).float() # test image array\n",
    "print(dummy_img.shape)\n",
    "\n",
    "req_features = []\n",
    "output = dummy_img.clone().to(DEVICE)\n",
    "\n",
    "for feature in features:\n",
    "    output = feature(output)\n",
    "#     print(output.size()) => torch.Size([batch_size, channel, width, height])\n",
    "    if output.size()[2] < 800//16: # 800/16=50\n",
    "        break\n",
    "    req_features.append(feature)\n",
    "    out_channels = output.size()[1]\n",
    "    \n",
    "print(len(req_features))\n",
    "# print(req_features)\n",
    "print(out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgHCU9G3OzaF"
   },
   "outputs": [],
   "source": [
    "# convert this list into a Seqeuntial module\n",
    "\n",
    "faster_rcnn_feature_extractor = nn.Sequential(*req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxBrHR7iOzaF",
    "outputId": "4ed28000-b26d-4e8a-c104-633682b17e4d"
   },
   "outputs": [],
   "source": [
    "# test the results of the input image pass through the feature extractor\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "imgTensor = transform(img).to(DEVICE)\n",
    "imgTensor = imgTensor.unsqueeze(0)\n",
    "output_map = faster_rcnn_feature_extractor(imgTensor)\n",
    "\n",
    "print(output_map.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "RQ9rXnuJOzaF",
    "outputId": "0fb3076d-0218-4886-b0ae-db9fd012963d"
   },
   "outputs": [],
   "source": [
    "# visualize the first 5 channels of the 50*50*512 feature maps\n",
    "\n",
    "imgArray = output_map.data.cpu().numpy().squeeze(0)\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "\n",
    "for i in range(5):\n",
    "    fig.add_subplot(1, 5, figNo)\n",
    "    plt.imshow(imgArray[i], cmap='gray')\n",
    "    figNo += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpdRugyNOzaF"
   },
   "source": [
    "## Generate Anchors Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeU1tbFVOzaF"
   },
   "source": [
    "### 1) Generate Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rr3641LOzaG",
    "outputId": "cbbd14e5-39a9-4ecc-ac7f-71bc99ce953d"
   },
   "outputs": [],
   "source": [
    "# sub-sampling rate = 1/16\n",
    "# image size : 800x800\n",
    "# sub-sampled feature map size : 800 x 1/16 = 50\n",
    "# 50 x 50 = 2500 anchors and each anchor generate 9 anchor boxes\n",
    "# total anchor boxes = 50 x 50 x 9 = 22500\n",
    "# x,y intervals to generate anchor box center\n",
    "\n",
    "feature_size = 800 // 16\n",
    "ctr_x = np.arange(16, (feature_size + 1) * 16, 16)\n",
    "ctr_y = np.arange(16, (feature_size + 1) * 16, 16)\n",
    "print(len(ctr_x))\n",
    "print(ctr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2bg707ROzaG",
    "outputId": "9f8ebdab-fea4-411f-c11c-fe849bab1723"
   },
   "outputs": [],
   "source": [
    "# coordinates of the 255 center points to generate anchor boxes\n",
    "\n",
    "index = 0\n",
    "ctr = np.zeros((2500, 2))\n",
    "\n",
    "for i in range(len(ctr_x)):\n",
    "    for j in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[i] - 8\n",
    "        ctr[index, 0] = ctr_y[j] - 8\n",
    "        index += 1\n",
    "\n",
    "# ctr => [[center x, center y], ...]\n",
    "print(ctr.shape)\n",
    "print(ctr[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "_gyTpFigOzaG",
    "outputId": "4c4d2edb-d3a5-4c74-adc9-2458c1222fac"
   },
   "outputs": [],
   "source": [
    "# display the 2500 anchors within image\n",
    "\n",
    "img_clone2 = np.copy(img)\n",
    "ctr_int = ctr.astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(ctr.shape[0]):\n",
    "    cv2.circle(img_clone2, (ctr_int[i][0], ctr_int[i][1]),\n",
    "              radius=1, color=(255, 0, 0), thickness=3)\n",
    "plt.imshow(img_clone2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D78VhgYfOzaH"
   },
   "source": [
    "### 2) Generate Anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcR2VtdeOzaH",
    "outputId": "4a9fd148-4ee0-4ede-f16e-fde27e8b74ad"
   },
   "outputs": [],
   "source": [
    "# for each of the 2500 anchors, generate 9 anchor boxes\n",
    "# 2500 x 9 = 22500 anchor boxes\n",
    "\n",
    "ratios = [0.5, 1, 2]\n",
    "scales = [8, 16, 32]\n",
    "sub_sample = 16\n",
    "\n",
    "anchor_boxes = np.zeros(((feature_size * feature_size * 9), 4))\n",
    "index = 0\n",
    "\n",
    "for c in ctr:                        # per anchors\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):     # per ratios\n",
    "        for j in range(len(scales)): # per scales\n",
    "            \n",
    "            # anchor box height, width\n",
    "            h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
    "            \n",
    "            # anchor box [x1, y1, x2, y2]\n",
    "            anchor_boxes[index, 1] = ctr_y - h / 2.\n",
    "            anchor_boxes[index, 0] = ctr_x - w / 2.\n",
    "            anchor_boxes[index, 3] = ctr_y + h / 2.\n",
    "            anchor_boxes[index, 2] = ctr_x + w / 2.\n",
    "            index += 1\n",
    "            \n",
    "print(anchor_boxes.shape)\n",
    "print(anchor_boxes[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "OoHqcOAbOzaH",
    "outputId": "692cf5a3-aed9-4ddb-bba9-3951d66c4602"
   },
   "outputs": [],
   "source": [
    "# display the anchor boxes of one anchor and the ground truth boxes\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "# draw random anchor boxes\n",
    "for i in range(11025, 11034):\n",
    "    x1 = int(anchor_boxes[i][0])\n",
    "    y1 = int(anchor_boxes[i][1])\n",
    "    x2 = int(anchor_boxes[i][2])\n",
    "    y2 = int(anchor_boxes[i][3])\n",
    "    \n",
    "    cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255, 0, 0),\n",
    "                 thickness=3)\n",
    "\n",
    "# draw ground truth boxes\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), \n",
    "                             (bbox[i][2], bbox[i][3]),\n",
    "                 color=(0, 255, 0), thickness=3)\n",
    "\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "dx--9RjaOzaI",
    "outputId": "e2c76e0f-b55f-4d24-befb-c2f808f7aaa9"
   },
   "outputs": [],
   "source": [
    "# draw all anchor boxes\n",
    "\n",
    "# add paddings(can't draw anchor boxes out of image boundary)\n",
    "img_clone3 = np.copy(img)\n",
    "img_clone4 = cv2.copyMakeBorder(img_clone3,400,400,400,400,cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "img_clone5 = np.copy(img_clone4)\n",
    "\n",
    "for i in range(len(anchor_boxes)):\n",
    "    x1 = int(anchor_boxes[i][0])\n",
    "    y1 = int(anchor_boxes[i][1])\n",
    "    x2 = int(anchor_boxes[i][2])\n",
    "    y2 = int(anchor_boxes[i][3])\n",
    "    \n",
    "    cv2.rectangle(img_clone5, (x1+400, y1+400), (x2+400, y2+400), color=(255, 0, 0),\n",
    "                 thickness=3)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121), plt.imshow(img_clone4)\n",
    "plt.subplot(122), plt.imshow(img_clone5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6fdAOPvOzaI"
   },
   "source": [
    "## Target Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbA4MWCtOzaI"
   },
   "source": [
    "### 1) Only choose anchor boxes inside the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwpFU_obOzaJ",
    "outputId": "ad0d3a1c-6d85-434e-f568-95b89e423a98"
   },
   "outputs": [],
   "source": [
    "# ignore the cross-boundary anchor boxes\n",
    "# valid anchor boxes with (x1, y1) > 0 and (x2, y2) <= 800\n",
    "\n",
    "index_inside = np.where(\n",
    "        (anchor_boxes[:, 0] >= 0) &\n",
    "        (anchor_boxes[:, 1] >= 0) &\n",
    "        (anchor_boxes[:, 2] <= 800) &\n",
    "        (anchor_boxes[:, 3] <= 800))[0]\n",
    "\n",
    "print(index_inside.shape)\n",
    "\n",
    "# only 8940 anchor boxes are inside the boundary out of 22500\n",
    "valid_anchor_boxes = anchor_boxes[index_inside]\n",
    "print(valid_anchor_boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZTT_d0KOzaJ"
   },
   "source": [
    "### 2) Calculate IoUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be47IUZfOzaJ",
    "outputId": "2d01e5cb-95a3-4af0-98b4-b3a2f181a851"
   },
   "outputs": [],
   "source": [
    "# calculate Iou of the valid anchor boxes\n",
    "# since we have 8940 anchor boxes and 4 ground truth objects,\n",
    "# we should get an array with (8940, 4) as the output\n",
    "# [IoU with gt box1, IoU with gt box2, IoU with gt box3,IoU with gt box4]\n",
    "\n",
    "ious = np.empty((len(valid_anchor_boxes),4), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "\n",
    "# anchor boxes\n",
    "for i, anchor_box in enumerate(valid_anchor_boxes):\n",
    "    xa1, ya1, xa2, ya2 = anchor_box\n",
    "    anchor_area = (xa2 - xa1) * (ya2 - ya1)\n",
    "    \n",
    "    # ground truth boxes\n",
    "    for j, gt_box in enumerate(bbox):\n",
    "        xb1, yb1, xb2, yb2 = gt_box\n",
    "        box_area = (xb2 - xb1) * (yb2 - yb1)\n",
    "        \n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        \n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
    "            iou = inter_area / (anchor_area + box_area - inter_area)\n",
    "        else:\n",
    "            iou = 0\n",
    "        \n",
    "        ious[i, j] = iou\n",
    "        \n",
    "print(ious.shape)\n",
    "print(ious[8930:8940, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDUEzUfvOzaJ"
   },
   "source": [
    "### 3) Sample positive/negative anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7mtX-z9OzaK",
    "outputId": "ad0e4be6-986e-49d0-f9ab-82ebb565ee4e"
   },
   "outputs": [],
   "source": [
    "# what anchor box has max ou with the ground truth box\n",
    "\n",
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)\n",
    "\n",
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mIDFhJzOzaK",
    "outputId": "b1222741-ab02-4b7e-f4a7-18dd15f8e1ba"
   },
   "outputs": [],
   "source": [
    "# what ground truth bbox is associated with each anchor box\n",
    "\n",
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bhXELufOzaK",
    "outputId": "a54f1762-576f-46ae-c1de-e47fe8956c4c"
   },
   "outputs": [],
   "source": [
    "# set the labels of 8940 valid anchor boxes to -1(ignore)\n",
    "\n",
    "label = np.empty((len(index_inside),), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkgYCi32OzaL"
   },
   "outputs": [],
   "source": [
    "# use IoU to assign 1 (objects) to two kind of anchors\n",
    "# a) the anchors with the highest IoU overlap with a ground truth box\n",
    "# b) an anchor that has an IoU overlap higher than 0.7 with ground truth box\n",
    "\n",
    "# Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3\n",
    "\n",
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3\n",
    "\n",
    "label[gt_argmax_ious] = 1\n",
    "label[max_ious >= pos_iou_threshold] = 1\n",
    "label[max_ious < neg_iou_threshold] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJn9NIEgOzaL"
   },
   "outputs": [],
   "source": [
    "# Every time mini-batch training take only 256 valid anchor boxes to train RPN\n",
    "# of which 128 positive examples, 128 negative-examples\n",
    "# disable leftover positive/negative anchors \n",
    "n_sample = 256\n",
    "pos_ratio = 0.5\n",
    "n_pos = pos_ratio * n_sample\n",
    "\n",
    "pos_index = np.where(label == 1)[0]\n",
    "\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index,\n",
    "                                    size = (len(pos_index) - n_pos),\n",
    "                                    replace=False)\n",
    "    label[disable_index] = -1\n",
    "    \n",
    "n_neg = n_sample * np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, \n",
    "                                    size = (len(neg_index) - n_neg), \n",
    "                                    replace = False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J63z4807OzaL",
    "outputId": "4c696e9a-421a-463f-e962-c04725fb88c3"
   },
   "outputs": [],
   "source": [
    "# convert the format of valid anchor boxes [x1, y1, x2, y2]\n",
    "\n",
    "# For each valid anchor box, find the groundtruth object which has max_iou \n",
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox.shape)\n",
    "\n",
    "height = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "width = valid_anchor_boxes[:, 2] = valid_anchor_boxes[:, 0]\n",
    "ctr_y = valid_anchor_boxes[:, 1] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 0] + 0.5 * width\n",
    "\n",
    "base_height = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_width = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_ctr_y = max_iou_bbox[:, 1] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 0] + 0.5 * base_width\n",
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "anchor_locs = np.vstack((dx, dy, dw, dh)).transpose()\n",
    "print(anchor_locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOX4fz2FOzaL",
    "outputId": "6c18af65-17b5-46a7-b4a7-ce13c4019f67"
   },
   "outputs": [],
   "source": [
    "# First set the label=-1 and locations=0 of the 22500 anchor boxes, \n",
    "# and then fill in the locations and labels of the 8940 valid anchor boxes\n",
    "# NOTICE: For each training epoch, we randomly select 128 positive + 128 negative \n",
    "# from 8940 valid anchor boxes, and the others are marked with -1\n",
    "\n",
    "anchor_labels = np.empty((len(anchor_boxes),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label\n",
    "print(anchor_labels.shape)\n",
    "print(anchor_labels[:10])\n",
    "\n",
    "anchor_locations = np.empty((len(anchor_boxes),) + anchor_boxes.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_locations[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUinQObVROe1"
   },
   "source": [
    "## RPN(Region Proposal Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBkuAN_gZse3"
   },
   "source": [
    "### 1) Define RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNlcWKMoQz8n",
    "outputId": "a7b42aef-e500-4188-a8f8-64c2c9696ef1"
   },
   "outputs": [],
   "source": [
    "# Send the features of the input image to the Region Proposal Network (RPN), \n",
    "# predict 22500 region proposals (ROIs)\n",
    "\n",
    "in_channels = 512\n",
    "mid_channels = 512\n",
    "n_anchor = 9\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(DEVICE)\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "# bounding box regressor\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0).to(DEVICE)\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "# classifier(object or not)\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0).to(DEVICE)\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0vcnjHIZwtP"
   },
   "source": [
    "### 2) Classification and Bounding box regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ma6FEg0zSEna",
    "outputId": "796d7145-0427-4d20-ac13-7eb1e1afd96e"
   },
   "outputs": [],
   "source": [
    "x = conv1(output_map.to(DEVICE)) # output_map = faster_rcnn_feature_extractor(imgTensor)\n",
    "pred_anchor_locs = reg_layer(x) # bounding box regresor output\n",
    "pred_cls_scores = cls_layer(x)  # classifier output \n",
    "\n",
    "print(pred_anchor_locs.shape, pred_cls_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMbYuRJcSomQ",
    "outputId": "3c0a8412-9e01-4563-c8aa-67ab83ca193c"
   },
   "outputs": [],
   "source": [
    "# Convert RPN to predict the position and classification format of the anchor box\n",
    "# Position: [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw) \n",
    "# Classification: [1, 18(9*2), 50, 50] => [1, 22500, 2] (1, 0)\n",
    "\n",
    "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(pred_anchor_locs.shape)\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "\n",
    "objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(objectness_score.shape)\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GajID3HT0sr",
    "outputId": "ddb8ee83-0e41-4341-ee57-73df7ae21102"
   },
   "outputs": [],
   "source": [
    "# According to the 22500 ROIs predicted by RPN and 22500 anchor boxes, \n",
    "# calculate the RPN loss¶\n",
    "print(pred_anchor_locs.shape)\n",
    "print(pred_cls_scores.shape)\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNwblnMJW1gy",
    "outputId": "b6c5e946-0b57-4514-da64-7f3b105e66f2"
   },
   "outputs": [],
   "source": [
    "rpn_loc = pred_anchor_locs[0]\n",
    "rpn_score = pred_cls_scores[0]\n",
    "\n",
    "gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "\n",
    "print(rpn_loc.shape, rpn_score.shape,\n",
    "      gt_rpn_loc.shape, gt_rpn_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ_TaMWEVd-Y"
   },
   "source": [
    "### 3) Multi-task loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7-EgBycXNlk",
    "outputId": "f116b4d8-18e7-461e-948d-e1d2ce0a9453"
   },
   "outputs": [],
   "source": [
    "# For classification we use cross-entropy loss\n",
    "rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long().to(DEVICE), ignore_index = -1)\n",
    "print(rpn_cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mYW_sJSX5e-",
    "outputId": "ada308d7-caf7-40ff-ae84-8a326cb8c268"
   },
   "outputs": [],
   "source": [
    "# only positive samples\n",
    "pos = gt_rpn_score > 0\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes whick have positive labels\n",
    "mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "rpn_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(rpn_loc_loss.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzRQoNzcYNzf",
    "outputId": "b798a40e-e004-4425-bdf3-d8a7a24d7f28"
   },
   "outputs": [],
   "source": [
    "# Combining both the rpn_cls_loss and rpn_reg_loss\n",
    "\n",
    "rpn_lambda = 10\n",
    "N_reg = (gt_rpn_score > 0).float().sum()\n",
    "rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
    "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\n",
    "print(rpn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvzI7kllVhmZ"
   },
   "source": [
    "## Proposal layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDShTOZtWYDQ"
   },
   "source": [
    "### 1) Transform anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiAgT7pcYOkY"
   },
   "outputs": [],
   "source": [
    "# Send the 22500 ROIs predicted by RPN to Fast RCNN to predict bbox + classifications\n",
    "# First use NMS (Non-maximum supression) to reduce 22500 ROI to 2000\n",
    "\n",
    "nms_thresh = 0.7  # non-maximum supression (NMS) \n",
    "n_train_pre_nms = 12000 # no. of train pre-NMS\n",
    "n_train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300 # During testing we evaluate 300 proposals,\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFiyeIpzgIll",
    "outputId": "2f0cfecc-d88f-4423-b154-b78ba974410b"
   },
   "outputs": [],
   "source": [
    "# the labelled 22500 anchor boxes\n",
    "# format converted from [x1, y1, x2, y2] to [ctrx, ctry, w, h]\n",
    "\n",
    "anc_height = anchor_boxes[:, 3] - anchor_boxes[:, 1]\n",
    "anc_width = anchor_boxes[:, 2] - anchor_boxes[:, 0]\n",
    "anc_ctr_y = anchor_boxes[:, 1] + 0.5 * anc_height\n",
    "anc_ctr_x = anchor_boxes[:, 0] + 0.5 * anc_width\n",
    "print(anc_ctr_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVBwaodoUVED",
    "outputId": "35413585-6385-42cb-d817-fef852bcd636"
   },
   "outputs": [],
   "source": [
    "# The 22500 anchor boxes location and labels predicted by RPN (convert to numpy)\n",
    "# format = (dx, dy, dw, dh)\n",
    "\n",
    "pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].cpu().data.numpy()\n",
    "\n",
    "dy = pred_anchor_locs_numpy[:, 1::4]\n",
    "dx = pred_anchor_locs_numpy[:, 0::4]\n",
    "dh = pred_anchor_locs_numpy[:, 3::4]\n",
    "dw = pred_anchor_locs_numpy[:, 2::4]\n",
    "print(dy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63ruoyK2VAsi",
    "outputId": "eec048dd-cd4c-49f7-a2b6-d0549f186453"
   },
   "outputs": [],
   "source": [
    "# ctr_y = dy predicted by RPN * anchor_h + anchor_cy\n",
    "# ctr_x similar\n",
    "# h = exp(dh predicted by RPN) * anchor_h\n",
    "# w similar\n",
    "\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cujhuRY5VUdQ",
    "outputId": "a3ae51b9-fb2a-4db2-d679-535c8d194b0d"
   },
   "outputs": [],
   "source": [
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=anchor_locs.dtype)\n",
    "roi[:, 0::4] = ctr_x - 0.5 * w\n",
    "roi[:, 1::4] = ctr_y - 0.5 * h\n",
    "roi[:, 2::4] = ctr_x + 0.5 * w\n",
    "roi[:, 3::4] = ctr_y + 0.5 * h\n",
    "\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EocfP-RoYEDD"
   },
   "source": [
    "### 2) Clip the anchor boxes to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbLWP7vXV7zM",
    "outputId": "1714ef0b-0c74-4d2b-80fb-3d2bd5a1559f"
   },
   "outputs": [],
   "source": [
    "# clip the predcited boxes to the image\n",
    "\n",
    "img_size = (800, 800)\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0]) # [:, 0, 2]\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1]) # [:, 1, 3]\n",
    "\n",
    "print(roi.shape, np.max(roi), np.min(roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtTUmM0PWO8-",
    "outputId": "9d668ccd-5260-477d-9715-d577918e39ea"
   },
   "outputs": [],
   "source": [
    "# remove predicted boxes with either height or width < threshold\n",
    "\n",
    "hs = roi[:, 3] - roi[:, 1]\n",
    "ws = roi[:, 2] - roi[:, 0]\n",
    "\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "print(keep.shape, roi.shape, score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP12Ml4HYI5q"
   },
   "source": [
    "### 3) Select top-12000 anchor boxes by objectness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkUlj6SzXEYq",
    "outputId": "3cb1465c-347c-4620-a95d-ee0a312968f7"
   },
   "outputs": [],
   "source": [
    "# sort all (proposal, score) pairs by score from highest to lowest\n",
    "\n",
    "order = score.ravel().argsort()[::-1]\n",
    "print(order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqGGS7ClXu0D",
    "outputId": "07c6097c-fd87-41f7-abac-b48486667a6f"
   },
   "outputs": [],
   "source": [
    "# take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "order = order[:n_train_pre_nms]\n",
    "roi = roi[order, :]\n",
    "print(order.shape, roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvZYnGDZZuB8"
   },
   "source": [
    "### 4) Non maximum suppression(select 2000 bounding boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SmohcWvX7Ny"
   },
   "outputs": [],
   "source": [
    "# take all the roi boxes\n",
    "x1 = roi[:, 0]\n",
    "y1 = roi[:, 1]\n",
    "x2 = roi[:, 2]\n",
    "y2 = roi[:, 3]\n",
    "\n",
    "# find the areas of all the boxes\n",
    "\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQ800DrMYq5r",
    "outputId": "eaf14993-01df-4782-c0d1-81f98ad43966"
   },
   "outputs": [],
   "source": [
    "# take the indexes of order the probability score in descending order\n",
    "# non maximum suppression\n",
    "\n",
    "order = order.argsort()[::-1]\n",
    "keep = []\n",
    "\n",
    "while (order.size > 0):\n",
    "  i = order[0] # take the 1st elt in roder and append to keep\n",
    "  keep.append(i)\n",
    "\n",
    "  xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "  yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "  xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "  yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "  w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "  h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "\n",
    "  inter = w * h\n",
    "  ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "  inds = np.where(ovr <= nms_thresh)[0]\n",
    "  order = order[inds + 1]\n",
    "\n",
    "keep = keep[:n_train_post_nms] # while training/testing, use accordingly\n",
    "roi = roi[keep]\n",
    "print(len(keep), roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHGUFCwgaI8s"
   },
   "source": [
    "## Proposal Target layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGoaq-3uetv1"
   },
   "source": [
    "### 1) Calculate IoUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRzuAEb4Zr96"
   },
   "outputs": [],
   "source": [
    "n_sample = 128 # number of samples from roi\n",
    "pos_ratio = 0.25 # number of positive examples out of the n_samples\n",
    "pos_iou_thresh = 0.5 # min iou of region proposal with any ground truth object to consider it as positive label\n",
    "neg_iou_thresh_hi = 0.5 # iou 0~0.5 is considered as negative (0, background)\n",
    "neg_iou_thresh_lo = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nb34dvb2atCd",
    "outputId": "e5112ab3-14d4-41ac-8324-ba112ad8bbc0"
   },
   "outputs": [],
   "source": [
    "# fine the iou of each ground truth object with the region proposals\n",
    "\n",
    "ious = np.empty((len(roi), bbox.shape[0]), dtype = np.float32)\n",
    "ious.fill(0)\n",
    "\n",
    "for num1, i in enumerate(roi):\n",
    "  ya1, xa1, ya2, xa2 = i\n",
    "  anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "\n",
    "  for num2, j in enumerate(bbox):\n",
    "    yb1, xb1, yb2, xb2 = j\n",
    "    box_area = (yb2 - yb1) * (xb2 - xb1)\n",
    "    inter_x1 = max([xb1, xa1])\n",
    "    inter_y1 = max([yb1, ya1])\n",
    "    inter_x2 = min([xb2, xa2])\n",
    "    inter_y2 = min([yb2, ya2])\n",
    "\n",
    "    if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "      inter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "      iou = inter_area / (anchor_area + box_area - inter_area)\n",
    "    else:\n",
    "      iou = 0\n",
    "    ious[num1, num2] = iou\n",
    "\n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJbzmZgab1wR",
    "outputId": "be375cc3-8563-481a-d529-6ac06ad3bf77"
   },
   "outputs": [],
   "source": [
    "# find out whick ground truth has high IoU for each region proposal\n",
    "# also find the maximum IoU\n",
    "\n",
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_iou = ious.max(axis=1)\n",
    "\n",
    "print(gt_assignment)\n",
    "print(max_iou)\n",
    "\n",
    "# assign the labels to each proposal\n",
    "gt_roi_label = labels[gt_assignment]\n",
    "print(gt_roi_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnAmZMYGewx4"
   },
   "source": [
    "### 2) Select foreground(positive) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyQT2EBxdGJn",
    "outputId": "3fbb6281-1164-483c-d2dd-2c5c0970ed71"
   },
   "outputs": [],
   "source": [
    "# select the foreground rois as pre the pos_iou_thresh\n",
    "# and n_sample x pos_ratio (128 x 0.25 = 32) foreground samples\n",
    "\n",
    "pos_roi_per_image = 32\n",
    "pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "\n",
    "if pos_index.size > 0:\n",
    "  pos_index = np.random.choice(\n",
    "      pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "  \n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2B3OALze3K7"
   },
   "source": [
    "### 3) Select background(negative) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EneAQklkdv4e",
    "outputId": "83ededc2-1711-416c-8776-b5f49da5f790"
   },
   "outputs": [],
   "source": [
    "# similarly we do for negative(background) region proposals\n",
    "\n",
    "neg_index = np.where((max_iou < neg_iou_thresh_hi) &\n",
    "                     (max_iou >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "\n",
    "if neg_index.size > 0:\n",
    "  neg_index = np.random.choice(\n",
    "    neg_index, size = neg_roi_per_this_image, replace=False)\n",
    "  \n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhVaImBVfk_c"
   },
   "source": [
    "### 4) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "uZnUrLDeejVS",
    "outputId": "78fcc4d5-3c4a-4581-8ce5-6bf93b0b0b81"
   },
   "outputs": [],
   "source": [
    "# display RoI samples with positive\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "for i in range(pos_roi_per_this_image):\n",
    "  x1, y1, x2, y2 = roi[pos_index[i]].astype(int)\n",
    "  cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255,255,255),\n",
    "                thickness=3)\n",
    "  \n",
    "for i in range(len(bbox)):\n",
    "  cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), \n",
    "                color = (0, 255, 0), thickness=3)\n",
    "\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "oLD1bje-fgt2",
    "outputId": "e778bb28-dcc0-493e-9919-0b22060401f2"
   },
   "outputs": [],
   "source": [
    "# display RoI samples with negative\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "for i in range(neg_roi_per_this_image):\n",
    "  x1, y1, x2, y2 = roi[neg_index[i]].astype(int)\n",
    "  cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255, 255, 255),\n",
    "                thickness=3)\n",
    "  \n",
    "for i in range(len(bbox)):\n",
    "  cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), \n",
    "                color = (0, 255, 0), thickness=3)\n",
    "  \n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmsYYXQ-hiw1"
   },
   "source": [
    "### 5) Gather positive/negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfIM24sdgXNe",
    "outputId": "820b7c4b-4b8f-4b9d-c1b2-811810162d6f"
   },
   "outputs": [],
   "source": [
    "# now we gather positive samples index and negative samples index\n",
    "# their respective labels and region proposals\n",
    "\n",
    "keep_index = np.append(pos_index, neg_index)\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0 # negative labels => 0\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZ3vATgQhL8j",
    "outputId": "d4482e8b-761a-44d7-c1f8-32597687e0ee"
   },
   "outputs": [],
   "source": [
    "# pick the ground truth objects for these sample_roi and\n",
    "# later parameterized as we have done while assigning locations to \n",
    "# anchor boxes\n",
    "\n",
    "bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK8ijEAwhetf"
   },
   "outputs": [],
   "source": [
    "width = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "height = sample_roi[:, 3] - sample_roi[:, 1]\n",
    "ctr_x = sample_roi[:, 0] + 0.5 * width\n",
    "ctr_y = sample_roi[:, 1] + 0.5 * height\n",
    "\n",
    "base_width = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_height = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 0] + 0.5 * base_width\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 1] + 0.5 * base_height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsbkGMmPiW88",
    "outputId": "f29c55f4-cf23-48a0-bae5-ec80d54342bd"
   },
   "outputs": [],
   "source": [
    "# transform anchor boxes\n",
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dw = np.log(base_width / width)\n",
    "dh = np.log(base_height / height)\n",
    "\n",
    "gt_roi_locs = np.vstack((dx, dy, dw, dh)).transpose()\n",
    "print(gt_roi_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grYFC2nBi6tG"
   },
   "source": [
    "## RoI pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXnHiLkfp0b6"
   },
   "source": [
    "### 1) Concatenate labels with bbox coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSkqC5JOi1KJ",
    "outputId": "5a733604-663b-41bb-d519-a0ba3f09de06"
   },
   "outputs": [],
   "source": [
    "# Take out the features of 128 ROI samples and \n",
    "# use max pooling to adjust to the same size, H=7, W=7 (ROI Pooling)\n",
    "\n",
    "rois = torch.from_numpy(sample_roi).float()\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "print(rois.shape, roi_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWV_JbnrkdhJ",
    "outputId": "b9dd7dde-6e3f-4a6f-870a-c934f725f0fb"
   },
   "outputs": [],
   "source": [
    "indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "print(xy_indices_and_rois.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmvDsNHCp5p0"
   },
   "source": [
    "### 2) RoI pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ck-2pgjekjW3"
   },
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YedQSETlLO0",
    "outputId": "84b963b0-2816-457c-883e-507484d48268"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "rois = indices_and_rois.data.float()\n",
    "rois[:, 1:].mul_(1/16.0) # sub-sampling ratio\n",
    "rois = rois.long()\n",
    "num_rois = rois.size(0)\n",
    "\n",
    "for i in range(num_rois):\n",
    "  roi = rois[i]\n",
    "  im_idx = roi[0]\n",
    "  im = output_map.narrow(0, im_idx, 1)[..., roi[1]:(roi[3]+1), roi[2]:(roi[4]+1)]\n",
    "  tmp = adaptive_max_pool(im)\n",
    "  output.append(tmp[0])\n",
    "\n",
    "output = torch.cat(output, 0)\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "Wz-OZ7q_mU1H",
    "outputId": "fd6e4164-cd66-4a64-b038-4b4d28100ad3"
   },
   "outputs": [],
   "source": [
    "# Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512)\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = output_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = im[0][0].detach().cpu().numpy()\n",
    "    fig.add_subplot(1, 5, figNo) \n",
    "    plt.imshow(tmp, cmap='gray')\n",
    "    \n",
    "    figNo +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTCW79Gvmayu",
    "outputId": "62a9fef7-1a01-4541-8d97-07367effe2e3"
   },
   "outputs": [],
   "source": [
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "k = output.view(output.size(0), -1)\n",
    "print(k.shape) # 25088 = 7*7*512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN5WVRCiqEv0"
   },
   "source": [
    "## Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb7m0xQFsuoH"
   },
   "source": [
    "### 1) Classifier and Bounding box regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieTjx33XmyQF"
   },
   "outputs": [],
   "source": [
    "# 128 boxes + features (7x7x512) of ROI samples are sent to \n",
    "# Detection network to predict the objects bounding box and clas of the input image\n",
    "\n",
    "roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096), nn.Linear(4096, 4096)]).to(DEVICE)\n",
    "cls_loc = nn.Linear(4096, 2 * 4).to(DEVICE) # 1 class, 1 background, 4 coordiinates\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "\n",
    "score = nn.Linear(4096, 2).to(DEVICE) # 1 class, 1 background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXIac3SZqtHn",
    "outputId": "d2c0fba2-ed9f-4fe2-d69e-6e5482425cdf"
   },
   "outputs": [],
   "source": [
    "# passing the output of roi pooling to RoI head\n",
    "\n",
    "k = roi_head_classifier(k.to(DEVICE))\n",
    "roi_cls_loc = cls_loc(k)\n",
    "roi_cls_score = score(k)\n",
    "\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y19CW0CarChb",
    "outputId": "25c4010d-f3dc-4d6b-cd30-5e12062fb1dc"
   },
   "outputs": [],
   "source": [
    "# Calculate the loss of Fast RCNN based on the gt bboxes and features (h, w, d=512) \n",
    "# corresponding to these 128 ROIs\n",
    "\n",
    "# predicted\n",
    "print(roi_cls_loc.shape)\n",
    "print(roi_cls_score.shape)\n",
    "\n",
    "#actual\n",
    "print(gt_roi_locs.shape)\n",
    "print(gt_roi_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nz5wb_TsrJlN",
    "outputId": "d78da495-672a-4faf-8e0b-f2df85c8a41c"
   },
   "outputs": [],
   "source": [
    "gt_roi_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVAnnleis0fA"
   },
   "source": [
    "### 2) Classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUxB0A8DrLhm",
    "outputId": "05b1e5bd-fdbd-455c-d573-7c06a8af697c"
   },
   "outputs": [],
   "source": [
    "# Converting ground truth to torch variable\n",
    "gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "print(gt_roi_loc.shape, gt_roi_label.shape)\n",
    "\n",
    "#Classification loss\n",
    "roi_cls_loss = F.cross_entropy(roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BtOIa3Rs3zG"
   },
   "source": [
    "### 3) Regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNp2fdPrrNOt",
    "outputId": "6ec9adbe-4843-4b22-edda-e563f4dd8597"
   },
   "outputs": [],
   "source": [
    "# regression loss\n",
    "\n",
    "n_sample = roi_cls_loc.shape[0]\n",
    "roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "print(roi_loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVk20rkxrkOW",
    "outputId": "7b4e7671-2e35-426c-a3a5-f1088ffe80e6"
   },
   "outputs": [],
   "source": [
    "roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2DMPpTTrs45",
    "outputId": "f24d5dfb-e9c9-477b-c533-958ae6138331"
   },
   "outputs": [],
   "source": [
    "# for regression we use smooth l1 loss as defined in the Fast R-CNN paper\n",
    "pos = gt_roi_label > 0\n",
    "mask = pos.unsqueeze(1).expand_as(roi_loc)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7R7D-TrSr3DN",
    "outputId": "b4e813ec-e81c-4c63-ab64-8bd46ba1a084"
   },
   "outputs": [],
   "source": [
    "# take those bounding boxes which have positive labels\n",
    "mask_loc_preds = roi_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_roi_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "roi_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(roi_loc_loss.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fdK1l2ks66p"
   },
   "source": [
    "### 4) Multi-task loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_lambda = 10.\n",
    "roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)\n",
    "print(roi_loss)\n",
    "\n",
    "total_loss = rpn_loss.cpu() + roi_loss\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48AI9J3stPuk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "faster_r_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28c0d215912440fa99b12023a66a26a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ec3d0bb6d244fbbb8f79d7a8035bdf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a8b1558c7a94aacbc6cb4053b5b57a9",
       "IPY_MODEL_86eb0a8fc8644e8bb513143b5399302a"
      ],
      "layout": "IPY_MODEL_daa3e6f30b6149f4a44c78e8ba143981"
     }
    },
    "828fcccc6d18425d904ac4e8d639e74b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86eb0a8fc8644e8bb513143b5399302a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c0d215912440fa99b12023a66a26a0",
      "placeholder": "​",
      "style": "IPY_MODEL_828fcccc6d18425d904ac4e8d639e74b",
      "value": " 528M/528M [00:22&lt;00:00, 24.4MB/s]"
     }
    },
    "8a8b1558c7a94aacbc6cb4053b5b57a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4fe9a8476ed4390b3c4a6a73fd1ca9b",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0597676159b4ef7a6596a2b933a3418",
      "value": 553433881
     }
    },
    "b4fe9a8476ed4390b3c4a6a73fd1ca9b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa3e6f30b6149f4a44c78e8ba143981": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0597676159b4ef7a6596a2b933a3418": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
