{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_imshow(title, image):\n",
    "    # convert the image frame BGR to RGB color space and display it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # define the base path to the *original* input dataset and then use\n",
    "    # the base path to derive the image and annotations directories\n",
    "    ORIG_BASE_PATH = \"raccoons\"\n",
    "    ORIG_IMAGES = os.path.sep.join([ORIG_BASE_PATH, \"images\"])\n",
    "    ORIG_ANNOTS = os.path.sep.join([ORIG_BASE_PATH, \"annotations\"])\n",
    "\n",
    "    # define the base path to the *new* dataset after running our dataset\n",
    "    # builder scripts and then use the base path to derive the paths to\n",
    "    # our output class label directories\n",
    "    BASE_PATH = \"dataset\"\n",
    "    POSITVE_PATH = os.path.sep.join([BASE_PATH, \"raccoon\"])\n",
    "    NEGATIVE_PATH = os.path.sep.join([BASE_PATH, \"no_raccoon\"])\n",
    "\n",
    "    # define the number of max proposals used when running selective\n",
    "    # search for (1) gathering training data and (2) performing inference\n",
    "    MAX_PROPOSALS = 2000\n",
    "    MAX_PROPOSALS_INFER = 200\n",
    "\n",
    "    # define the maximum number of positive and negative images to be\n",
    "    # generated from each image\n",
    "    MAX_POSITIVE = 30\n",
    "    MAX_NEGATIVE = 10\n",
    "\n",
    "    # initialize the input dimensions to the network\n",
    "    INPUT_DIMS = (224, 224)\n",
    "    # define the path to the output model and label binarizer\n",
    "    MODEL_PATH = \"raccoon_detector.h5\"\n",
    "    ENCODER_PATH = \"label_encoder.pickle\"\n",
    "\n",
    "    # define the minimum probability required for a positive prediction\n",
    "    # (used to filter out false-positive predictions)\n",
    "    MIN_PROBA = 0.99\n",
    "\n",
    "# instantiate our Config object\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "boxA = np.array([100,100,299,299])\n",
    "boxB = np.array([200,200,399,399])\n",
    "iou = compute_iou(boxA, boxB)\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "boxA = np.array([0,0,199,199])\n",
    "boxB = np.array([300,0,499,199])\n",
    "iou = compute_iou(boxA, boxB)\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8223234624145785\n"
     ]
    }
   ],
   "source": [
    "boxA = np.array([0,0,199,199])\n",
    "boxB = np.array([10,10,209,209])\n",
    "iou = compute_iou(boxA, boxB)\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the output positive and negative directories\n",
    "for dirPath in (config.POSITVE_PATH, config.NEGATIVE_PATH):\n",
    "    # if the output directory does not exist yet, create it\n",
    "    if not os.path.exists(dirPath):\n",
    "        os.makedirs(dirPath)\n",
    "\n",
    "# grab all image paths in the input images directory\n",
    "imagePaths = list(paths.list_images(config.ORIG_IMAGES))\n",
    "\n",
    "# initialize the total number of positive and negative images we have\n",
    "# saved to disk so far\n",
    "totalPositive = 0\n",
    "totalNegative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing image 1/99...\n",
      "[INFO] processing image 2/99...\n",
      "[INFO] processing image 3/99...\n",
      "[INFO] processing image 4/99...\n",
      "[INFO] processing image 5/99...\n",
      "[INFO] processing image 6/99...\n",
      "[INFO] processing image 7/99...\n",
      "[INFO] processing image 8/99...\n",
      "[INFO] processing image 9/99...\n",
      "[INFO] processing image 10/99...\n",
      "[INFO] processing image 11/99...\n",
      "[INFO] processing image 12/99...\n",
      "[INFO] processing image 13/99...\n",
      "[INFO] processing image 14/99...\n",
      "[INFO] processing image 15/99...\n",
      "[INFO] processing image 16/99...\n",
      "[INFO] processing image 17/99...\n",
      "[INFO] processing image 18/99...\n",
      "[INFO] processing image 19/99...\n",
      "[INFO] processing image 20/99...\n",
      "[INFO] processing image 21/99...\n",
      "[INFO] processing image 22/99...\n",
      "[INFO] processing image 23/99...\n",
      "[INFO] processing image 24/99...\n",
      "[INFO] processing image 25/99...\n",
      "[INFO] processing image 26/99...\n",
      "[INFO] processing image 27/99...\n",
      "[INFO] processing image 28/99...\n",
      "[INFO] processing image 29/99...\n",
      "[INFO] processing image 30/99...\n",
      "[INFO] processing image 31/99...\n",
      "[INFO] processing image 32/99...\n",
      "[INFO] processing image 33/99...\n",
      "[INFO] processing image 34/99...\n",
      "[INFO] processing image 35/99...\n",
      "[INFO] processing image 36/99...\n",
      "[INFO] processing image 37/99...\n",
      "[INFO] processing image 38/99...\n",
      "[INFO] processing image 39/99...\n",
      "[INFO] processing image 40/99...\n",
      "[INFO] processing image 41/99...\n",
      "[INFO] processing image 42/99...\n",
      "[INFO] processing image 43/99...\n",
      "[INFO] processing image 44/99...\n",
      "[INFO] processing image 45/99...\n",
      "[INFO] processing image 46/99...\n",
      "[INFO] processing image 47/99...\n",
      "[INFO] processing image 48/99...\n",
      "[INFO] processing image 49/99...\n",
      "[INFO] processing image 50/99...\n",
      "[INFO] processing image 51/99...\n",
      "[INFO] processing image 52/99...\n",
      "[INFO] processing image 53/99...\n",
      "[INFO] processing image 54/99...\n",
      "[INFO] processing image 55/99...\n",
      "[INFO] processing image 56/99...\n",
      "[INFO] processing image 57/99...\n",
      "[INFO] processing image 58/99...\n",
      "[INFO] processing image 59/99...\n",
      "[INFO] processing image 60/99...\n",
      "[INFO] processing image 61/99...\n",
      "[INFO] processing image 62/99...\n",
      "[INFO] processing image 63/99...\n",
      "[INFO] processing image 64/99...\n",
      "[INFO] processing image 65/99...\n",
      "[INFO] processing image 66/99...\n",
      "[INFO] processing image 67/99...\n",
      "[INFO] processing image 68/99...\n",
      "[INFO] processing image 69/99...\n",
      "[INFO] processing image 70/99...\n",
      "[INFO] processing image 71/99...\n",
      "[INFO] processing image 72/99...\n",
      "[INFO] processing image 73/99...\n",
      "[INFO] processing image 74/99...\n",
      "[INFO] processing image 75/99...\n",
      "[INFO] processing image 76/99...\n",
      "[INFO] processing image 77/99...\n",
      "[INFO] processing image 78/99...\n",
      "[INFO] processing image 79/99...\n",
      "[INFO] processing image 80/99...\n",
      "[INFO] processing image 81/99...\n",
      "[INFO] processing image 82/99...\n",
      "[INFO] processing image 83/99...\n",
      "[INFO] processing image 84/99...\n",
      "[INFO] processing image 85/99...\n",
      "[INFO] processing image 86/99...\n",
      "[INFO] processing image 87/99...\n",
      "[INFO] processing image 88/99...\n",
      "[INFO] processing image 89/99...\n",
      "[INFO] processing image 90/99...\n",
      "[INFO] processing image 91/99...\n",
      "[INFO] processing image 92/99...\n",
      "[INFO] processing image 93/99...\n",
      "[INFO] processing image 94/99...\n",
      "[INFO] processing image 95/99...\n",
      "[INFO] processing image 96/99...\n",
      "[INFO] processing image 97/99...\n",
      "[INFO] processing image 98/99...\n",
      "[INFO] processing image 99/99...\n"
     ]
    }
   ],
   "source": [
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # show a progress report\n",
    "    print(\"[INFO] processing image {}/{}...\".format(i + 1,\n",
    "        len(imagePaths)))\n",
    "\n",
    "    # extract the filename from the file path and use it to derive\n",
    "    # the path to the XML annotation file\n",
    "    filename = imagePath.split(os.path.sep)[-1]\n",
    "    filename = filename[:filename.rfind(\".\")]\n",
    "    annotPath = os.path.sep.join([config.ORIG_ANNOTS,\n",
    "        \"{}.xml\".format(filename)])\n",
    "\n",
    "    # load the annotation file, build the soup, and initialize our\n",
    "    # list of ground-truth bounding boxes\n",
    "    contents = open(annotPath).read()\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    gtBoxes = []\n",
    "\n",
    "    # extract the image dimensions\n",
    "    w = int(soup.find(\"width\").string)\n",
    "    h = int(soup.find(\"height\").string)\n",
    "\n",
    "    # loop over all 'object' elements\n",
    "    for o in soup.find_all(\"object\"):\n",
    "        # extract the label and bounding box coordinates\n",
    "        label = o.find(\"name\").string\n",
    "        xMin = int(o.find(\"xmin\").string)\n",
    "        yMin = int(o.find(\"ymin\").string)\n",
    "        xMax = int(o.find(\"xmax\").string)\n",
    "        yMax = int(o.find(\"ymax\").string)\n",
    "\n",
    "        # truncate any bounding box coordinates that may fall\n",
    "        # outside the boundaries of the image\n",
    "        xMin = max(0, xMin)\n",
    "        yMin = max(0, yMin)\n",
    "        xMax = min(w, xMax)\n",
    "        yMax = min(h, yMax)\n",
    "\n",
    "        # update our list of ground-truth bounding boxes\n",
    "        gtBoxes.append((xMin, yMin, xMax, yMax))\n",
    "\n",
    "    # load the input image from disk\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    # run selective search on the image and initialize our list of\n",
    "    # proposed boxes\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    proposedRects= []\n",
    "\n",
    "    # loop over the rectangles generated by selective search\n",
    "    for (x, y, w, h) in rects:\n",
    "        # convert our bounding boxes from (x, y, w, h) to (startX,\n",
    "        # startY, startX, endY)\n",
    "        proposedRects.append((x, y, x + w, y + h))\n",
    "\n",
    "    # initialize counters used to count the number of positive and\n",
    "    # negative ROIs saved thus far\n",
    "    positiveROIs = 0\n",
    "    negativeROIs = 0\n",
    "\n",
    "    # loop over the maximum number of region proposals\n",
    "    for proposedRect in proposedRects[:config.MAX_PROPOSALS]:\n",
    "        # unpack the proposed rectangle bounding box\n",
    "        (propStartX, propStartY, propEndX, propEndY) = proposedRect\n",
    "\n",
    "        # loop over the ground-truth bounding boxes\n",
    "        for gtBox in gtBoxes:\n",
    "            # compute the intersection over union between the two\n",
    "            # boxes and unpack the ground-truth bounding box\n",
    "            iou = compute_iou(gtBox, proposedRect)\n",
    "            (gtStartX, gtStartY, gtEndX, gtEndY) = gtBox\n",
    "\n",
    "            # initialize the ROI and output path\n",
    "            roi = None\n",
    "            outputPath = None\n",
    "\n",
    "            # check to see if the IOU is greater than 70% *and* that\n",
    "            # we have not hit our positive count limit\n",
    "            if iou > 0.7 and positiveROIs <= config.MAX_POSITIVE:\n",
    "                # extract the ROI and then derive the output path to\n",
    "                # the positive instance\n",
    "                roi = image[propStartY:propEndY, propStartX:propEndX]\n",
    "                filename = \"{}.png\".format(totalPositive)\n",
    "                outputPath = os.path.sep.join([config.POSITVE_PATH,\n",
    "                    filename])\n",
    "\n",
    "                # increment the positive counters\n",
    "                positiveROIs += 1\n",
    "                totalPositive += 1\n",
    "\n",
    "            # determine if the proposed bounding box falls *within*\n",
    "            # the ground-truth bounding box\n",
    "            fullOverlap = propStartX >= gtStartX\n",
    "            fullOverlap = fullOverlap and propStartY >= gtStartY\n",
    "            fullOverlap = fullOverlap and propEndX <= gtEndX\n",
    "            fullOverlap = fullOverlap and propEndY <= gtEndY\n",
    "\n",
    "            # check to see if there is not full overlap *and* the IoU\n",
    "            # is less than 5% *and* we have not hit our negative\n",
    "            # count limit\n",
    "            if not fullOverlap and iou < 0.05 and \\\n",
    "                negativeROIs <= config.MAX_NEGATIVE:\n",
    "                # extract the ROI and then derive the output path to\n",
    "                # the negative instance\n",
    "                roi = image[propStartY:propEndY, propStartX:propEndX]\n",
    "                filename = \"{}.png\".format(totalNegative)\n",
    "                outputPath = os.path.sep.join([config.NEGATIVE_PATH,\n",
    "                    filename])\n",
    "\n",
    "                # increment the negative counters\n",
    "                negativeROIs += 1\n",
    "                totalNegative += 1\n",
    "\n",
    "            # check to see if both the ROI and output path are valid\n",
    "            if roi is not None and outputPath is not None:\n",
    "                # resize the ROI to the input dimensions of the CNN\n",
    "                # that we'll be fine-tuning, then write the ROI to\n",
    "                # disk\n",
    "                roi = cv2.resize(roi, config.INPUT_DIMS,\n",
    "                    interpolation=cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(outputPath, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"plot\": \"plot.png\"\n",
    "}\n",
    "\n",
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 3\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(config.BASE_PATH))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    # load the input image (224x224) and preprocess it\n",
    "    image = load_img(imagePath, target_size=config.INPUT_DIMS)\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.20, stratify=labels, random_state=42)\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
    "# left off\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "    target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving mask detector model...\")\n",
    "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
    "\n",
    "# serialize the label encoder to disk\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(config.ENCODER_PATH, \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"image\": \"images/raccoon_02.jpg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the our fine-tuned model and label binarizer from disk\n",
    "print(\"[INFO] loading model and label binarizer...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "lb = pickle.loads(open(config.ENCODER_PATH, \"rb\").read())\n",
    "\n",
    "# load the input image from disk\n",
    "image = cv2.imread(args[\"image\"])\n",
    "image = imutils.resize(image, width=500)\n",
    "\n",
    "# run selective search on the image to generate bounding box proposal\n",
    "# regions\n",
    "print(\"[INFO] running selective search...\")\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(image)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "rects = ss.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of region proposals that we'll be classifying\n",
    "# along with their associated bounding boxes\n",
    "proposals = []\n",
    "boxes = []\n",
    "\n",
    "# loop over the region proposal bounding box coordinates generated by\n",
    "# running selective search\n",
    "for (x, y, w, h) in rects[:config.MAX_PROPOSALS_INFER]:\n",
    "    # extract the region from the input image, convert it from BGR to\n",
    "    # RGB channel ordering, and then resize it to the required input\n",
    "    # dimensions of our trained CNN\n",
    "    roi = image[y:y + h, x:x + w]\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    roi = cv2.resize(roi, config.INPUT_DIMS,\n",
    "        interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # further preprocess the ROI\n",
    "    roi = img_to_array(roi)\n",
    "    roi = preprocess_input(roi)\n",
    "\n",
    "    # update our proposals and bounding boxes lists\n",
    "    proposals.append(roi)\n",
    "    boxes.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the proposals and bounding boxes into NumPy arrays\n",
    "proposals = np.array(proposals, dtype=\"float32\")\n",
    "boxes = np.array(boxes, dtype=\"int32\")\n",
    "print(\"[INFO] proposal shape: {}\".format(proposals.shape))\n",
    "\n",
    "# classify each of the proposal ROIs using fine-tuned model\n",
    "print(\"[INFO] classifying proposals...\")\n",
    "proba = model.predict(proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of all predictions that are positive for the\n",
    "# \"raccoon\" class\n",
    "print(\"[INFO] applying NMS...\")\n",
    "labels = lb.classes_[np.argmax(proba, axis=1)]\n",
    "idxs = np.where(labels == \"raccoon\")[0]\n",
    "\n",
    "# use the indexes to extract all bounding boxes and associated class\n",
    "# label probabilities associated with the \"raccoon\" class\n",
    "boxes = boxes[idxs]\n",
    "proba = proba[idxs][:, 1]\n",
    "\n",
    "# further filter indexes by enforcing a minimum prediction\n",
    "# probability be met\n",
    "idxs = np.where(proba >= config.MIN_PROBA)\n",
    "boxes = boxes[idxs]\n",
    "proba = proba[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the original image so that we can draw on it\n",
    "clone = image.copy()\n",
    "\n",
    "# loop over the bounding boxes and associated probabilities\n",
    "for (box, prob) in zip(boxes, proba):\n",
    "    # draw the bounding box, label, and probability on the image\n",
    "    (startX, startY, endX, endY) = box\n",
    "    cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "        (0, 255, 0), 2)\n",
    "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    text= \"Raccoon: {:.2f}%\".format(prob * 100)\n",
    "    cv2.putText(clone, text, (startX, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "# show the output after *before* running NMS\n",
    "plt_imshow(\"Before NMS\", clone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, probs=None, overlapThresh=0.3):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes are integers, convert them to floats -- this\n",
    "    # is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # compute the area of the bounding boxes and grab the indexes to sort\n",
    "    # (in the case that no probabilities are provided, simply sort on the\n",
    "    # bottom-left y-coordinate)\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = y2\n",
    "\n",
    "    # if probabilities are provided, sort on them instead\n",
    "    if probs is not None:\n",
    "        idxs = probs\n",
    "\n",
    "    # sort the indexes\n",
    "    idxs = np.argsort(idxs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the index value\n",
    "        # to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of the bounding\n",
    "        # box and the smallest (x, y) coordinates for the end of the bounding\n",
    "        # box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have overlap greater\n",
    "        # than the provided overlap threshold\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return the indexes of only the bounding boxes to keep\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run non-maxima suppression on the bounding boxes\n",
    "boxIdxs = non_max_suppression(boxes, proba)\n",
    "\n",
    "# loop over the bounding box indexes\n",
    "for i in boxIdxs:\n",
    "    # draw the bounding box, label, and probability on the image\n",
    "    (startX, startY, endX, endY) = boxes[i]\n",
    "    cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "        (0, 255, 0), 2)\n",
    "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    text= \"Raccoon: {:.2f}%\".format(proba[i] * 100)\n",
    "    cv2.putText(image, text, (startX, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "# show the output image *after* running NMS\n",
    "plt_imshow(\"After NMS\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
