{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
      "metadata": {
        "id": "a2e7d62b-5779-4211-822c-457c77321f8b"
      },
      "source": [
        "# OpenVINO™를 사용하여 YOLOv8 실시간 개체 감지를 변환하고 최적화합니다.\n",
        "\n",
        "실시간 객체 감지는 종종 컴퓨터 비전 시스템의 핵심 구성 요소로 사용됩니다.\n",
        "실시간 객체 감지 모델을 사용하는 애플리케이션에는 비디오 분석, 로봇 공학, 자율 주행 차량, 다중 객체 추적 및 객체 계산, 의료 이미지 분석 등이 포함됩니다.\n",
        "\n",
        "\n",
        "이 튜토리얼에서는 OpenVINO를 사용하여 PyTorch YOLOv8을 실행하고 최적화하는 방법에 대한 단계별 지침을 보여줍니다. 객체 감지 시나리오에 필요한 단계를 고려합니다.\n",
        "\n",
        "튜토리얼은 다음 단계로 구성됩니다.\n",
        "- PyTorch 모델을 준비합니다.\n",
        "- 데이터 세트를 다운로드하고 준비합니다.\n",
        "- 원본 모델을 검증합니다.\n",
        "- PyTorch 모델을 OpenVINO IR로 변환합니다.\n",
        "- 변환된 모델을 검증합니다.\n",
        "- 최적화 파이프라인을 준비하고 실행합니다.\n",
        "- FP32와 양자화 모델의 성능을 비교합니다.\n",
        "- FP32와 양자화 모델의 정확도를 비교합니다.\n",
        "- OpenVINO API를 통한 기타 최적화 가능성\n",
        "- 라이브 데모\n",
        "\n",
        "\n",
        "#### 내용의 테이블:\n",
        "- [PyTorch 모델 가져오기](#Get-PyTorch-model-Uparrow)\n",
        "     - [선행조건](#선행조건-Uparrow)\n",
        "- [모델 인스턴스화](#Instantiate-model-Uparrow)\n",
        "     - [모델을 OpenVINO IR로 변환](#Convert-model-to-OpenVINO-IR-Uparrow)\n",
        "     - [모델 추론 검증](#Verify-model-inference-Uparrow)\n",
        "     - [전처리](#전처리-Uparrow)\n",
        "     - [후처리](#후처리-Uparrow)\n",
        "     - [추론 장치 선택](#Select-inference-device-Uparrow)\n",
        "     - [단일 이미지 테스트](#Test-on-single-image-Uparrow)\n",
        "- [데이터세트의 모델 정확도 확인](#Check-model-accuracy-on-the-dataset-Uparrow)\n",
        "     - [검증 데이터 세트 다운로드](#Download-the-validation-dataset-Uparrow)\n",
        "     - [검증 함수 정의](#Define-validation-function-Uparrow)\n",
        "     - [검사기 도우미 구성 및 DataLoader 생성](#Configure-Validator-helper-and-create-DataLoader-Uparrow)\n",
        "- [NNCF 사후 훈련 양자화 API를 사용하여 모델 최적화](#Optimize-model-using-NNCF-Post-training-Quantization-API-Uparrow)\n",
        "     - [양자화 모델 추론 검증](#Validate-Quantized-model-inference-Uparrow)\n",
        "- [원본 모델과 양자화 모델 비교](#Compare-the-Original-and-Quantized-Models-Uparrow)\n",
        "     - [성능 객체 감지 모델 비교](#Compare-performance-object-Detection-models-Uparrow)\n",
        "     - [양자화 모델 정확도 검증](#Validate-Quantized-model-accuracy-Uparrow)\n",
        "- [다음 단계](#다음-단계-위쪽 화살표)\n",
        "     - [비동기 추론 파이프라인](#Async-inference-pipeline-Uparrow)\n",
        "     - [모델로의 통합 전처리](#Integration-preprocessing-to-model-Uparrow)\n",
        "         - [PrePostProcessing API 초기화](#Initialize-PrePostProcessing-API-Uparrow)\n",
        "         - [입력 데이터 형식 정의](#Define-input-data-format-Uparrow)\n",
        "         - [전처리 단계 설명](#Describe-preprocessing-steps-Uparrow)\n",
        "         - [모델에 단계 통합](#Integrating-Steps-into-a-Model-Uparrow)\n",
        "- [라이브 데모](#Live-demo-Uparrow)\n",
        "     - [라이브 객체 감지 실행](#Run-Live-Object-Detection-Uparrow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a12678-b12f-48d1-9735-398855733e46",
      "metadata": {
        "id": "d7a12678-b12f-48d1-9735-398855733e46"
      },
      "source": [
        "## PyTorch 모델 가져오기 [$\\Uparrow$](#목차:)\n",
        "\n",
        "일반적으로 PyTorch 모델은 모델이 포함된 상태 사전에 의해 초기화된 [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) 클래스의 인스턴스를 나타냅니다. 무게.\n",
        "우리는 이 [repo](https://github.com/ultralytics/ultralytics)에서 사용할 수 있는 COCO 데이터세트에서 사전 훈련된 YOLOv8 나노 모델('yolov8n'이라고도 함)을 사용합니다. 비슷한 단계가 다른 YOLOv8 모델에도 적용 가능합니다.\n",
        "사전 훈련된 모델을 얻기 위한 일반적인 단계:\n",
        "1. 모델 클래스의 인스턴스를 생성합니다.\n",
        "2. 사전 훈련된 모델 가중치가 포함된 체크포인트 상태 사전을 로드합니다.\n",
        "3. 일부 작업을 추론 모드로 전환하기 위해 모델을 평가로 전환합니다.\n",
        "\n",
        "이 경우 모델 작성자는 YOLOv8 모델을 ONNX로 변환한 다음 OpenVINO IR로 변환할 수 있는 API를 제공합니다. 따라서 이러한 단계를 수동으로 수행할 필요가 없습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2267760-cbfe-41c6-958d-cad9f845d5bb",
      "metadata": {
        "id": "e2267760-cbfe-41c6-958d-cad9f845d5bb"
      },
      "source": [
        "#### 전제조건 [$\\Uparrow$](#목차:)\n",
        "\n",
        "필요한 패키지를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d04872-6916-454c-9211-6c644b50dc04",
      "metadata": {
        "id": "30d04872-6916-454c-9211-6c644b50dc04"
      },
      "outputs": [],
      "source": [
        "%pip install -q \"openvino>=2023.1.0\" \"nncf>=2.5.0\"\n",
        "%pip install -q \"ultralytics==8.0.43\" onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bbe319c",
      "metadata": {
        "id": "1bbe319c"
      },
      "source": [
        "필수 유틸리티 기능을 가져옵니다.\n",
        "아래쪽 셀은 GitHub에서 'notebook_utils' Python 모듈을 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f6cd89",
      "metadata": {
        "id": "a2f6cd89"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fetch the notebook utils script from the openvino_notebooks repo\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(\n",
        "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
        "    filename='notebook_utils.py'\n",
        ")\n",
        "\n",
        "from notebook_utils import download_file, VideoPlayer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f758b3a",
      "metadata": {
        "id": "9f758b3a"
      },
      "source": [
        "결과 그리기를 위한 유틸리티 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917755b4-435a-41a0-a427-a8152cd3cc50",
      "metadata": {
        "id": "917755b4-435a-41a0-a427-a8152cd3cc50"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics.yolo.utils.plotting import colors\n",
        "\n",
        "\n",
        "def plot_one_box(box:np.ndarray, img:np.ndarray,\n",
        "                 color:Tuple[int, int, int] = None,\n",
        "                 label:str = None, line_thickness:int = 5):\n",
        "    \"\"\"\n",
        "    Helper function for drawing single bounding box on image\n",
        "    Parameters:\n",
        "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
        "        img (no.ndarray): input image\n",
        "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
        "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
        "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
        "    \"\"\"\n",
        "    # Plots one bounding box on image img\n",
        "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
        "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "    if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def draw_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
        "    \"\"\"\n",
        "    Helper function for drawing bounding boxes on image\n",
        "    Parameters:\n",
        "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
        "        source_image (np.ndarray): input image for drawing\n",
        "        label_map; (Dict[int, str]): label_id to class name mapping\n",
        "    Returns:\n",
        "        Image with boxes\n",
        "    \"\"\"\n",
        "    boxes = results[\"det\"]\n",
        "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
        "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
        "        source_image = plot_one_box(xyxy, source_image, label=label, color=colors(int(lbl)), line_thickness=1)\n",
        "    return source_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373658bd-7e64-4479-914e-f2742d330afd",
      "metadata": {
        "id": "373658bd-7e64-4479-914e-f2742d330afd"
      },
      "outputs": [],
      "source": [
        "# Download a test sample\n",
        "IMAGE_PATH = Path('./data/coco_bike.jpg')\n",
        "download_file(\n",
        "    url='https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg',\n",
        "    filename=IMAGE_PATH.name,\n",
        "    directory=IMAGE_PATH.parent\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
      "metadata": {
        "id": "ee32fd08-650c-4751-bb41-d8afccb2495e"
      },
      "source": [
        "## 모델 인스턴스화 [$\\Uparrow$](#목차:)\n",
        "\n",
        "원본 저장소에는 다양한 작업을 대상으로 하는 [여러 모델](https://docs.ultralytics.com/tasks/Detect/)이 있습니다. 모델을 로드하려면 모델 체크포인트에 대한 경로를 지정해야 합니다. 모델 허브에서 사용할 수 있는 일부 로컬 경로 또는 이름일 수 있습니다(이 경우 모델 체크포인트는 자동으로 다운로드됩니다).\n",
        "\n",
        "예측을 수행하면 모델은 입력 이미지에 대한 경로를 수락하고 Results 클래스 개체가 포함된 목록을 반환합니다. 결과에는 객체 감지 모델에 대한 상자가 포함됩니다. 또한 결과를 처리하기 위한 유틸리티(예: 그리기를 위한 `plot()` 메서드)도 포함되어 있습니다.\n",
        "\n",
        "다음의 예를 고려해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae2543b",
      "metadata": {
        "id": "bae2543b"
      },
      "outputs": [],
      "source": [
        "models_dir = Path('./models')\n",
        "models_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
      "metadata": {
        "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "DET_MODEL_NAME = \"yolov8n\"\n",
        "\n",
        "det_model = YOLO(models_dir / f'{DET_MODEL_NAME}.pt')\n",
        "label_map = det_model.model.names\n",
        "\n",
        "res = det_model(IMAGE_PATH)\n",
        "Image.fromarray(res[0].plot()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
      "metadata": {
        "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da"
      },
      "source": [
        "### 모델을 OpenVINO IR로 변환 [$\\Uparrow$](#목차:)\n",
        "\n",
        "YOLOv8은 OpenVINO IR을 포함한 다양한 형식으로 편리한 모델을 내보낼 수 있는 API를 제공합니다. `model.export`는 모델 변환을 담당합니다. 형식을 지정해야 하며 추가적으로 모델의 동적 형태를 보존할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
      "metadata": {
        "id": "bf2cc576-50c9-409f-be86-ad7122dce24b"
      },
      "outputs": [],
      "source": [
        "# object detection model\n",
        "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
        "if not det_model_path.exists():\n",
        "    det_model.export(format=\"openvino\", dynamic=True, half=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713cd45f-2b19-4a1e-bc9d-5e69bd95d896",
      "metadata": {
        "id": "713cd45f-2b19-4a1e-bc9d-5e69bd95d896"
      },
      "source": [
        "### 모델 추론 확인 [$\\Uparrow$](#목차:)\n",
        "\n",
        "모델 작업을 테스트하기 위해 'model.predict' 메서드와 유사한 추론 파이프라인을 생성합니다. 파이프라인은 전처리 단계, OpenVINO 모델 추론, 결과를 얻기 위한 결과 후처리로 구성됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b8f151e-0dfe-4005-bee6-49130b519854",
      "metadata": {
        "id": "7b8f151e-0dfe-4005-bee6-49130b519854"
      },
      "source": [
        "### 전처리 [$\\Uparrow$](#목차:)\n",
        "\n",
        "모델 입력은 `N, C, H, W` 형식의 `[-1, 3, -1, -1]` 모양의 텐서입니다.\n",
        "* `N` - 배치의 이미지 수(배치 크기)\n",
        "* `C` - 이미지 채널\n",
        "* `H` - 이미지 높이\n",
        "* `W` - 이미지 너비\n",
        "\n",
        "모델은 RGB 채널 형식의 이미지를 예상하고 [0, 1] 범위로 정규화됩니다. 모델은 입력 분할성을 32로 유지하면서 동적 입력 형태를 지원하지만 효율성을 높이기 위해 정적 형태(예: 640x640)를 사용하는 것이 좋습니다. 모델 크기 `letterbox`에 맞게 이미지 크기를 조정하려면 너비와 높이의 종횡비가 유지되는 크기 조정 접근 방식이 사용됩니다.\n",
        "\n",
        "특정 모양을 유지하기 위해 전처리를 통해 자동으로 패딩이 활성화됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0598cb3a-e62c-4ef7-a0f4-9ec1fe0d5ca5",
      "metadata": {
        "id": "0598cb3a-e62c-4ef7-a0f4-9ec1fe0d5ca5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "from ultralytics.yolo.utils import ops\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def letterbox(img: np.ndarray, new_shape:Tuple[int, int] = (640, 640), color:Tuple[int, int, int] = (114, 114, 114), auto:bool = False, scale_fill:bool = False, scaleup:bool = False, stride:int = 32):\n",
        "    \"\"\"\n",
        "    Resize image and padding for detection. Takes image as input,\n",
        "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
        "\n",
        "    Parameters:\n",
        "      img (np.ndarray): image for preprocessing\n",
        "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
        "      color (Tuple(int, int, int)): color for filling padded area\n",
        "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
        "      scale_fill (bool): scale image to fill new_shape\n",
        "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
        "      stride (int): input padding stride\n",
        "    Returns:\n",
        "      img (np.ndarray): image after preprocessing\n",
        "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
        "      padding_size (Tuple(int, int)): height and width padding size\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scale_fill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)\n",
        "\n",
        "\n",
        "def preprocess_image(img0: np.ndarray):\n",
        "    \"\"\"\n",
        "    Preprocess image according to YOLOv8 input requirements.\n",
        "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
        "\n",
        "    Parameters:\n",
        "      img0 (np.ndarray): image for preprocessing\n",
        "    Returns:\n",
        "      img (np.ndarray): image after preprocessing\n",
        "    \"\"\"\n",
        "    # resize\n",
        "    img = letterbox(img0)[0]\n",
        "\n",
        "    # Convert HWC to CHW\n",
        "    img = img.transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def image_to_tensor(image:np.ndarray):\n",
        "    \"\"\"\n",
        "    Preprocess image according to YOLOv8 input requirements.\n",
        "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
        "\n",
        "    Parameters:\n",
        "      img (np.ndarray): image for preprocessing\n",
        "    Returns:\n",
        "      input_tensor (np.ndarray): input tensor in NCHW format with float32 values in [0, 1] range\n",
        "    \"\"\"\n",
        "    input_tensor = image.astype(np.float32)  # uint8 to fp32\n",
        "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "\n",
        "    # add batch dimension\n",
        "    if input_tensor.ndim == 3:\n",
        "        input_tensor = np.expand_dims(input_tensor, 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87408242-70c2-4774-8e68-fcf79ae0df6e",
      "metadata": {
        "id": "87408242-70c2-4774-8e68-fcf79ae0df6e"
      },
      "source": [
        "### 후처리 [$\\Uparrow$](#목차:)\n",
        "\n",
        "모델 출력에는 탐지 상자 후보가 포함되어 있습니다. 이는 'B,84,N' 형식의 '[-1,84,-1]' 모양을 가진 텐서입니다. 여기서:\n",
        "\n",
        "- `B` - 배치 크기\n",
        "- `N` - 감지 상자 수\n",
        "\n",
        "최종 예측을 얻으려면 최대가 아닌 억제 알고리즘을 적용하고 상자 좌표를 원래 이미지 크기로 다시 조정해야 합니다.\n",
        "\n",
        "마지막으로 감지 상자에는 [`x`, `y`, `h`, `w`, `class_no_1`, ..., `class_no_80`] 형식이 있습니다.\n",
        "\n",
        "- (`x`, `y`) - 상자 중심의 원시 좌표\n",
        "- `h`, `w` - 상자의 원래 높이와 너비\n",
        "- `class_no_1`, ..., `class_no_80` - 클래스에 대한 확률 분포입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e0c681",
      "metadata": {
        "id": "f9e0c681"
      },
      "outputs": [],
      "source": [
        "def postprocess(\n",
        "    pred_boxes:np.ndarray,\n",
        "    input_hw:Tuple[int, int],\n",
        "    orig_img:np.ndarray,\n",
        "    min_conf_threshold:float = 0.25,\n",
        "    nms_iou_threshold:float = 0.7,\n",
        "    agnosting_nms:bool = False,\n",
        "    max_detections:int = 300,\n",
        "):\n",
        "    \"\"\"\n",
        "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
        "    Parameters:\n",
        "        pred_boxes (np.ndarray): model output prediction boxes\n",
        "        input_hw (np.ndarray): preprocessed image\n",
        "        orig_image (np.ndarray): image before preprocessing\n",
        "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
        "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
        "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
        "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
        "    Returns:\n",
        "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label]\n",
        "    \"\"\"\n",
        "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\":max_detections}\n",
        "    preds = ops.non_max_suppression(\n",
        "        torch.from_numpy(pred_boxes),\n",
        "        min_conf_threshold,\n",
        "        nms_iou_threshold,\n",
        "        nc=80,\n",
        "        **nms_kwargs\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for i, pred in enumerate(preds):\n",
        "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
        "        if not len(pred):\n",
        "            results.append({\"det\": [], \"segment\": []})\n",
        "            continue\n",
        "        pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
        "        results.append({\"det\": pred})\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462338cc",
      "metadata": {
        "id": "462338cc"
      },
      "source": [
        "### 추론 장치 선택 [$\\Uparrow$](#목차:)\n",
        "\n",
        "OpenVINO를 사용하여 추론을 실행하려면 드롭다운 목록에서 장치를 선택하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e49089e",
      "metadata": {
        "id": "0e49089e"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "import openvino as ov\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value='AUTO',\n",
        "    description='Device:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd163eab-e803-4ae8-96da-22c3bf303630",
      "metadata": {
        "id": "cd163eab-e803-4ae8-96da-22c3bf303630"
      },
      "source": [
        "### 단일 이미지에 대한 테스트 [$\\Uparrow$](#목차:)\n",
        "\n",
        "이제 전처리 및 후처리 단계를 정의했으면 객체 감지를 위한 모델 예측을 확인할 준비가 되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97b41c0-02d4-4357-90a1-9b727f0fcf39",
      "metadata": {
        "id": "f97b41c0-02d4-4357-90a1-9b727f0fcf39"
      },
      "outputs": [],
      "source": [
        "core = ov.Core()\n",
        "\n",
        "det_ov_model = core.read_model(det_model_path)\n",
        "if device.value != \"CPU\":\n",
        "    det_ov_model.reshape({0: [1, 3, 640, 640]})\n",
        "det_compiled_model = core.compile_model(det_ov_model, device.value)\n",
        "\n",
        "\n",
        "def detect(image:np.ndarray, model:ov.Model):\n",
        "    \"\"\"\n",
        "    OpenVINO YOLOv8 model inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
        "    Parameters:\n",
        "        image (np.ndarray): input image.\n",
        "        model (Model): OpenVINO compiled model.\n",
        "    Returns:\n",
        "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
        "    \"\"\"\n",
        "    preprocessed_image = preprocess_image(image)\n",
        "    input_tensor = image_to_tensor(preprocessed_image)\n",
        "    result = model(input_tensor)\n",
        "    boxes = result[model.output(0)]\n",
        "    input_hw = input_tensor.shape[2:]\n",
        "    detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image)\n",
        "    return detections\n",
        "\n",
        "input_image = np.array(Image.open(IMAGE_PATH))\n",
        "detections = detect(input_image, det_compiled_model)[0]\n",
        "image_with_boxes = draw_results(detections, input_image, label_map)\n",
        "\n",
        "Image.fromarray(image_with_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5b0855-4bcd-46cc-a0a8-f13a693922e9",
      "metadata": {
        "id": "ae5b0855-4bcd-46cc-a0a8-f13a693922e9"
      },
      "source": [
        "## 데이터세트 [$\\Uparrow$](#목차:)에서 모델 정확도를 확인하세요.\n",
        "\n",
        "최적화된 모델 결과를 원본과 비교하려면 검증 데이터 세트의 모델 정확도 측면에서 측정 가능한 몇 가지 결과를 아는 것이 좋습니다.\n",
        "\n",
        "\n",
        "### 검증 데이터 세트 다운로드 [$\\Uparrow$](#목차:)\n",
        "\n",
        "YOLOv8은 COCO 데이터세트에 대해 사전 훈련되었으므로 모델 정확도를 평가하려면 이를 다운로드해야 합니다. YOLOv8 저장소에 제공된 지침에 따라 원래 모델 평가 기능과 함께 사용하려면 모델 작성자가 사용하는 형식으로 주석을 다운로드해야 합니다.\n",
        "\n",
        ">**참고**: 초기 데이터 세트 다운로드를 완료하는 데 몇 분 정도 걸릴 수 있습니다. 다운로드 속도는 인터넷 연결 품질에 따라 달라집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a3a3a2-5141-4cfd-bbf1-84dbb5e0bc08",
      "metadata": {
        "id": "33a3a3a2-5141-4cfd-bbf1-84dbb5e0bc08"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "DATA_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "LABELS_URL = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
        "CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/8ebe94d1e928687feaa1fee6d5668987df5e43be/ultralytics/datasets/coco.yaml\"\n",
        "\n",
        "OUT_DIR = Path('./datasets')\n",
        "\n",
        "DATA_PATH = OUT_DIR / \"val2017.zip\"\n",
        "LABELS_PATH = OUT_DIR / \"coco2017labels-segments.zip\"\n",
        "CFG_PATH = OUT_DIR / \"coco.yaml\"\n",
        "\n",
        "download_file(DATA_URL, DATA_PATH.name, DATA_PATH.parent)\n",
        "download_file(LABELS_URL, LABELS_PATH.name, LABELS_PATH.parent)\n",
        "download_file(CFG_URL, CFG_PATH.name, CFG_PATH.parent)\n",
        "\n",
        "if not (OUT_DIR / \"coco/labels\").exists():\n",
        "    with ZipFile(LABELS_PATH , \"r\") as zip_ref:\n",
        "        zip_ref.extractall(OUT_DIR)\n",
        "    with ZipFile(DATA_PATH , \"r\") as zip_ref:\n",
        "        zip_ref.extractall(OUT_DIR / 'coco/images')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f516ba2-e54c-416c-893a-cbcd2228e140",
      "metadata": {
        "id": "0f516ba2-e54c-416c-893a-cbcd2228e140"
      },
      "source": [
        "### 유효성 검사 함수 정의 [$\\Uparrow$](#목차:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96bd01e-5b92-49be-b7b8-9269ace1398b",
      "metadata": {
        "test_replace": {
          "int = None": "int = 100"
        },
        "id": "a96bd01e-5b92-49be-b7b8-9269ace1398b"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from ultralytics.yolo.utils.metrics import ConfusionMatrix\n",
        "\n",
        "\n",
        "def test(model:ov.Model, core:ov.Core, data_loader:torch.utils.data.DataLoader, validator, num_samples:int = None):\n",
        "    \"\"\"\n",
        "    OpenVINO YOLOv8 model accuracy validation function. Runs model validation on dataset and returns metrics\n",
        "    Parameters:\n",
        "        model (Model): OpenVINO model\n",
        "        data_loader (torch.utils.data.DataLoader): dataset loader\n",
        "        validator: instance of validator class\n",
        "        num_samples (int, *optional*, None): validate model only on specified number samples, if provided\n",
        "    Returns:\n",
        "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
        "    \"\"\"\n",
        "    validator.seen = 0\n",
        "    validator.jdict = []\n",
        "    validator.stats = []\n",
        "    validator.batch_i = 1\n",
        "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
        "    model.reshape({0: [1, 3, -1, -1]})\n",
        "    compiled_model = core.compile_model(model)\n",
        "    for batch_i, batch in enumerate(tqdm(data_loader, total=num_samples)):\n",
        "        if num_samples is not None and batch_i == num_samples:\n",
        "            break\n",
        "        batch = validator.preprocess(batch)\n",
        "        results = compiled_model(batch[\"img\"])\n",
        "        preds = torch.from_numpy(results[compiled_model.output(0)])\n",
        "        preds = validator.postprocess(preds)\n",
        "        validator.update_metrics(preds, batch)\n",
        "    stats = validator.get_stats()\n",
        "    return stats\n",
        "\n",
        "\n",
        "def print_stats(stats:np.ndarray, total_images:int, total_objects:int):\n",
        "    \"\"\"\n",
        "    Helper function for printing accuracy statistic\n",
        "    Parameters:\n",
        "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
        "        total_images (int) -  number of evaluated images\n",
        "        total objects (int)\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"Boxes:\")\n",
        "    mp, mr, map50, mean_ap = stats['metrics/precision(B)'], stats['metrics/recall(B)'], stats['metrics/mAP50(B)'], stats['metrics/mAP50-95(B)']\n",
        "    # Print results\n",
        "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'Precision', 'Recall', 'mAP@.5', 'mAP@.5:.95')\n",
        "    print(s)\n",
        "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
        "    print(pf % ('all', total_images, total_objects, mp, mr, map50, mean_ap))\n",
        "    if 'metrics/precision(M)' in stats:\n",
        "        s_mp, s_mr, s_map50, s_mean_ap = stats['metrics/precision(M)'], stats['metrics/recall(M)'], stats['metrics/mAP50(M)'], stats['metrics/mAP50-95(M)']\n",
        "        # Print results\n",
        "        s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'Precision', 'Recall', 'mAP@.5', 'mAP@.5:.95')\n",
        "        print(s)\n",
        "        pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
        "        print(pf % ('all', total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ee1a66-c2ca-413b-883c-497f718337e6",
      "metadata": {
        "id": "41ee1a66-c2ca-413b-883c-497f718337e6"
      },
      "source": [
        "### 유효성 검사 도우미 구성 및 DataLoader 생성 [$\\Uparrow$](#Table-of-content:)\n",
        "\n",
        "원본 모델 저장소는 정확성 검증 파이프라인을 나타내는 'Validator' 래퍼를 사용합니다. 데이터로더와 평가 지표를 생성하고 데이터로더에서 생성된 각 데이터 배치에 대한 지표를 업데이트합니다. 그 외에도 데이터 전처리와 결과 후처리를 담당합니다. 클래스 초기화를 위해서는 구성을 제공해야 합니다. 기본 설정을 사용하지만 사용자 정의 데이터를 테스트하기 위해 재정의하는 일부 매개변수로 대체할 수 있습니다. 모델은 유효성 검사기 클래스 인스턴스를 생성하는 'ValidatorClass' 메서드를 연결했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878473e3-7638-4471-bd8e-31350b119f22",
      "metadata": {
        "id": "878473e3-7638-4471-bd8e-31350b119f22"
      },
      "outputs": [],
      "source": [
        "from ultralytics.yolo.utils import DEFAULT_CFG\n",
        "from ultralytics.yolo.cfg import get_cfg\n",
        "from ultralytics.yolo.data.utils import check_det_dataset\n",
        "\n",
        "args = get_cfg(cfg=DEFAULT_CFG)\n",
        "args.data = str(CFG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466729b5-4fad-4de2-a1d4-2952718cbe7a",
      "metadata": {
        "id": "466729b5-4fad-4de2-a1d4-2952718cbe7a"
      },
      "outputs": [],
      "source": [
        "det_validator = det_model.ValidatorClass(args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccd7ba9-6066-4328-9563-4f1a740bca9d",
      "metadata": {
        "id": "fccd7ba9-6066-4328-9563-4f1a740bca9d"
      },
      "outputs": [],
      "source": [
        "det_validator.data = check_det_dataset(args.data)\n",
        "det_data_loader = det_validator.get_dataloader(\"datasets/coco\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808a8b42-4ca0-429f-b40b-4384c83aad61",
      "metadata": {
        "id": "808a8b42-4ca0-429f-b40b-4384c83aad61"
      },
      "outputs": [],
      "source": [
        "det_validator.is_coco = True\n",
        "det_validator.class_map = ops.coco80_to_coco91_class()\n",
        "det_validator.names = det_model.model.names\n",
        "det_validator.metrics.names = det_validator.names\n",
        "det_validator.nc = det_model.model.model[-1].nc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a73bbc-d0e0-494e-9533-26d51295a196",
      "metadata": {
        "id": "87a73bbc-d0e0-494e-9533-26d51295a196"
      },
      "source": [
        "정의 테스트 기능 및 유효성 검사기 생성 후 정확도 측정항목을 얻을 준비가 되었습니다.\n",
        ">**참고**: 모델 평가는 시간이 많이 걸리는 프로세스이며 하드웨어에 따라 몇 분 정도 걸릴 수 있습니다. 계산 시간을 줄이기 위해 'num_samples' 매개변수를 평가 하위 집합 크기로 정의하지만 이 경우 검증 하위 집합 차이로 인해 모델 작성자가 원래 보고한 정확도와 비교할 수 없을 수 있습니다.\n",
        "*전체 데이터 세트 `NUM_TEST_SAMPLES = None`에서 모델을 검증합니다.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e7bb1e",
      "metadata": {
        "id": "b5e7bb1e"
      },
      "outputs": [],
      "source": [
        "NUM_TEST_SAMPLES = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d2df4b-4e03-4c5d-a329-9fae35fdf7f7",
      "metadata": {
        "id": "f1d2df4b-4e03-4c5d-a329-9fae35fdf7f7"
      },
      "outputs": [],
      "source": [
        "fp_det_stats = test(det_ov_model, core, det_data_loader, det_validator, num_samples=NUM_TEST_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b396bb40-9c0c-48c3-a485-c4642dc6bf1f",
      "metadata": {
        "id": "b396bb40-9c0c-48c3-a485-c4642dc6bf1f"
      },
      "outputs": [],
      "source": [
        "print_stats(fp_det_stats, det_validator.seen, det_validator.nt_per_class.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7f89f3-4a8f-4b1c-a582-964e111e51dc",
      "metadata": {
        "id": "8d7f89f3-4a8f-4b1c-a582-964e111e51dc"
      },
      "source": [
        "`print_stats`는 다음과 같은 정확도 지표 목록을 보고합니다.\n",
        "\n",
        "* '정밀도'는 해당 객체만 식별하는 모델의 정확성 정도입니다.\n",
        "* 'Recall'은 모든 실제 객체를 감지하는 모델의 능력을 측정합니다.\n",
        "* `mAP@t` - 데이터 세트의 모든 클래스에 대해 집계된 Precision-Recall 곡선 아래 영역으로 표시되는 평균 정밀도를 의미합니다. 여기서 `t`는 IOU(Intersection Over Union) 임계값, 실제와 예측 사이의 중첩 정도입니다. 사물. 따라서 'mAP@.5'는 평균 정밀도가 0.5 IOU 임계값에서 계산됨을 나타내고, 'mAP@.5:.95'는 0.05 단계를 통해 0.5에서 0.95까지의 IOU 임계값 범위에서 계산됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b69e12-2e22-44c1-bfed-fdc88f6c424d",
      "metadata": {
        "id": "13b69e12-2e22-44c1-bfed-fdc88f6c424d"
      },
      "source": [
        "## NNCF 사후 훈련 양자화 API [$\\Uparrow$](#Table-of-content:)를 사용하여 모델 최적화\n",
        "\n",
        "[NNCF](https://github.com/openvinotoolkit/nncf)는 OpenVINO에서 정확도 저하를 최소화하면서 신경망 추론 최적화를 위한 고급 알고리즘 제품군을 제공합니다.\n",
        "YOLOv8을 최적화하기 위해 사후 훈련 모드(미세 조정 파이프라인 없이)에서 8비트 양자화를 사용합니다.\n",
        "\n",
        "최적화 프로세스에는 다음 단계가 포함됩니다.\n",
        "\n",
        "1. 양자화를 위한 데이터 세트를 생성합니다.\n",
        "2. 최적화된 모델을 얻으려면 'nncf.Quantize'를 실행하세요.\n",
        "3. 'openvino.runtime.serialize' 함수를 사용하여 OpenVINO IR 모델을 직렬화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7eeae8-f3f4-4e95-b1a6-a23085f03ebc",
      "metadata": {
        "id": "7d7eeae8-f3f4-4e95-b1a6-a23085f03ebc"
      },
      "source": [
        "양자화 정확도 테스트에 검증 데이터로더를 재사용합니다.\n",
        "이를 위해서는 `nncf.Dataset` 객체로 래핑하고 입력 텐서만 가져오기 위한 변환 함수를 정의해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd994958-6988-4a1d-ac7f-3efbd97135cc",
      "metadata": {
        "id": "fd994958-6988-4a1d-ac7f-3efbd97135cc"
      },
      "outputs": [],
      "source": [
        "import nncf  # noqa: F811\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def transform_fn(data_item:Dict):\n",
        "    \"\"\"\n",
        "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
        "    Parameters:\n",
        "       data_item: Dict with data item produced by DataLoader during iteration\n",
        "    Returns:\n",
        "        input_tensor: Input data for quantization\n",
        "    \"\"\"\n",
        "    input_tensor = det_validator.preprocess(data_item)['img'].numpy()\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "quantization_dataset = nncf.Dataset(det_data_loader, transform_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91629284-e261-494d-9464-146ed7190084",
      "metadata": {
        "id": "91629284-e261-494d-9464-146ed7190084"
      },
      "source": [
        "'nncf.Quantize' 함수는 모델 양자화를 위한 인터페이스를 제공합니다. OpenVINO 모델 및 양자화 데이터 세트의 인스턴스가 필요합니다.\n",
        "선택적으로 구성 양자화 프로세스를 위한 일부 추가 매개변수(양자화를 위한 샘플 수, 사전 설정, 무시된 범위 등)를 제공할 수 있습니다. YOLOv8 모델에는 활성화의 비대칭 양자화가 필요한 비ReLU 활성화 함수가 포함되어 있습니다. 더 나은 결과를 얻기 위해 '혼합' 양자화 사전 설정을 사용합니다. 이는 가중치의 대칭 양자화와 활성화의 비대칭 양자화를 제공합니다. 보다 정확한 결과를 얻으려면 `ignored_scope` 매개변수를 사용하여 후처리 하위 그래프의 작업을 부동 소수점 정밀도로 유지해야 합니다.\n",
        "\n",
        ">**참고**: 모델 학습 후 양자화는 시간이 많이 걸리는 프로세스입니다. 인내심을 가지세요. 하드웨어에 따라 몇 분 정도 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e8cec4-d0b3-4da0-b54c-7964e7bcbfe2",
      "metadata": {
        "id": "b2e8cec4-d0b3-4da0-b54c-7964e7bcbfe2"
      },
      "outputs": [],
      "source": [
        "ignored_scope = nncf.IgnoredScope(\n",
        "    types=[\"Multiply\", \"Subtract\", \"Sigmoid\"],  # ignore operations\n",
        "    names=[\n",
        "        \"/model.22/dfl/conv/Conv\",           # in the post-processing subgraph\n",
        "        \"/model.22/Add\",\n",
        "        \"/model.22/Add_1\",\n",
        "        \"/model.22/Add_2\",\n",
        "        \"/model.22/Add_3\",\n",
        "        \"/model.22/Add_4\",\n",
        "        \"/model.22/Add_5\",\n",
        "        \"/model.22/Add_6\",\n",
        "        \"/model.22/Add_7\",\n",
        "        \"/model.22/Add_8\",\n",
        "        \"/model.22/Add_9\",\n",
        "        \"/model.22/Add_10\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Detection model\n",
        "quantized_det_model = nncf.quantize(\n",
        "    det_ov_model,\n",
        "    quantization_dataset,\n",
        "    preset=nncf.QuantizationPreset.MIXED,\n",
        "    ignored_scope=ignored_scope\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab02faf1-196c-4f91-93d1-c86b4cae93d1",
      "metadata": {
        "id": "ab02faf1-196c-4f91-93d1-c86b4cae93d1"
      },
      "outputs": [],
      "source": [
        "from openvino.runtime import serialize\n",
        "int8_model_det_path = models_dir / f'{DET_MODEL_NAME}_openvino_int8_model/{DET_MODEL_NAME}.xml'\n",
        "print(f\"Quantized detection model will be saved to {int8_model_det_path}\")\n",
        "serialize(quantized_det_model, str(int8_model_det_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c92a68-3eb8-4ea5-9e35-d496f45b3cc3",
      "metadata": {
        "id": "c8c92a68-3eb8-4ea5-9e35-d496f45b3cc3"
      },
      "source": [
        "### 양자화 모델 추론 검증 [$\\Uparrow$](#목차:)\n",
        "\n",
        "`nncf.Quantize`는 예측을 위해 장치에 로드하는 데 적합한 OpenVINO 모델 클래스 인스턴스를 반환합니다. `INT8` 모델 입력 데이터 및 출력 결과 형식은 부동 소수점 모델 표현과 차이가 없습니다. 따라서 이미지에서 'INT8' 모델 결과를 얻기 위해 위에서 정의한 것과 동일한 'Detect' 함수를 재사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b24588",
      "metadata": {
        "id": "f1b24588"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2273f037-d091-4d63-af1e-eed6b41fa6e6",
      "metadata": {
        "id": "2273f037-d091-4d63-af1e-eed6b41fa6e6"
      },
      "outputs": [],
      "source": [
        "if device.value != \"CPU\":\n",
        "    quantized_det_model.reshape({0: [1, 3, 640, 640]})\n",
        "quantized_det_compiled_model = core.compile_model(quantized_det_model, device.value)\n",
        "input_image = np.array(Image.open(IMAGE_PATH))\n",
        "detections = detect(input_image, quantized_det_compiled_model)[0]\n",
        "image_with_boxes = draw_results(detections, input_image, label_map)\n",
        "\n",
        "Image.fromarray(image_with_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59081cae-9c92-46f0-8117-ed8c6fa6ff19",
      "metadata": {
        "id": "59081cae-9c92-46f0-8117-ed8c6fa6ff19"
      },
      "source": [
        "## 원본 모델과 양자화 모델 비교 [$\\Uparrow$](#목차:)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd927bc-d8f2-4cd0-95f1-88d977c4b4e3",
      "metadata": {
        "id": "0cd927bc-d8f2-4cd0-95f1-88d977c4b4e3"
      },
      "source": [
        "### 성능 개체 감지 모델 비교 [$\\Uparrow$](#목차:)\n",
        "\n",
        "마지막으로 OpenVINO [벤치마크 도구](https://docs.openvino.ai/2023.0/openvino_inference_engine_tools_benchmark_tool_README.html)를 사용하여 'FP32' 및 'INT8' 모델의 추론 성능을 측정합니다.\n",
        "\n",
        "> **참고**: 보다 정확한 성능을 위해서는 다른 애플리케이션을 닫은 후 터미널/명령 프롬프트에서 `benchmark_app`을 실행하는 것이 좋습니다. `benchmark_app -m <model_path> -d CPU -shape \"<input_shape>\"`를 실행하여 특정 입력 데이터 형태에 대한 CPU의 비동기 추론을 1분 동안 벤치마킹합니다. GPU를 벤치마킹하려면 'CPU'를 'GPU'로 변경하세요. 모든 명령줄 옵션의 개요를 보려면 `benchmark_app --help`를 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d249cbc9",
      "metadata": {
        "id": "d249cbc9"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b07129-ec21-40e0-9885-1928ace8a55a",
      "metadata": {
        "id": "b9b07129-ec21-40e0-9885-1928ace8a55a"
      },
      "outputs": [],
      "source": [
        "# Inference FP32 model (OpenVINO IR)\n",
        "!benchmark_app -m $det_model_path -d $device.value -api async -shape \"[1,3,640,640]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646cafce-8042-4b94-aead-de804b2051a2",
      "metadata": {
        "id": "646cafce-8042-4b94-aead-de804b2051a2"
      },
      "outputs": [],
      "source": [
        "# Inference INT8 model (OpenVINO IR)\n",
        "!benchmark_app -m $int8_model_det_path -d $device.value -api async -shape \"[1,3,640,640]\" -t 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f800b975-d001-4947-ac20-6be9fd737a95",
      "metadata": {
        "id": "f800b975-d001-4947-ac20-6be9fd737a95"
      },
      "source": [
        "### 양자화된 모델 정확도 검증 [$\\Uparrow$](#목차:)\n",
        "\n",
        "보시다시피 단일 이미지 테스트에서는 `INT8`과 부동소수점 모델 결과 사이에 큰 차이가 없습니다. 양자화가 모델 예측 정밀도에 어떻게 영향을 미치는지 이해하기 위해 데이터 세트에서 모델 정확도를 비교할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48299ffc-99b0-4b27-b365-6b0b8beabd59",
      "metadata": {
        "id": "48299ffc-99b0-4b27-b365-6b0b8beabd59"
      },
      "outputs": [],
      "source": [
        "int8_det_stats = test(quantized_det_model, core, det_data_loader, det_validator, num_samples=NUM_TEST_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150e5f85-cf0f-4362-9c9e-7be8957ccc70",
      "metadata": {
        "id": "150e5f85-cf0f-4362-9c9e-7be8957ccc70"
      },
      "outputs": [],
      "source": [
        "print(\"FP32 model accuracy\")\n",
        "print_stats(fp_det_stats, det_validator.seen, det_validator.nt_per_class.sum())\n",
        "\n",
        "print(\"INT8 model accuracy\")\n",
        "print_stats(int8_det_stats, det_validator.seen, det_validator.nt_per_class.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a8dd76-3de3-4d4c-baef-bfd90400b127",
      "metadata": {
        "id": "d4a8dd76-3de3-4d4c-baef-bfd90400b127"
      },
      "source": [
        "엄청난! 정확도가 변경된 것처럼 보이지만 크게 변경되지 않았으며 통과 기준을 충족합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184bcebd-588d-4b4d-a60a-0fa753a07ef9",
      "metadata": {
        "id": "184bcebd-588d-4b4d-a60a-0fa753a07ef9"
      },
      "source": [
        "## 다음 단계 [$\\Uparrow$](#목차:)\n",
        "이 섹션에는 OpenVINO를 사용하여 애플리케이션 성능을 추가로 향상시키는 방법에 대한 제안 사항이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710f5266-a114-4180-8da2-f13b33995a99",
      "metadata": {
        "id": "710f5266-a114-4180-8da2-f13b33995a99"
      },
      "source": [
        "### 비동기 추론 파이프라인 [$\\Uparrow$](#목차:)\n",
        "Async API의 주요 장점은 장치가 추론으로 바쁜 경우 애플리케이션이 현재 추론이 먼저 완료될 때까지 기다리지 않고 다른 작업(예: 입력 채우기 또는 다른 요청 예약)을 병렬로 수행할 수 있다는 것입니다. openvino를 사용하여 비동기 추론을 수행하는 방법을 이해하려면 [Async API 튜토리얼](../115-async-api/115-async-api.ipynb)을 참조하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7826c42b-778e-4505-86b8-ebf1bbe14d19",
      "metadata": {
        "id": "7826c42b-778e-4505-86b8-ebf1bbe14d19"
      },
      "source": [
        "### 모델 [$\\Uparrow$](#목차:)에 대한 통합 전처리\n",
        "\n",
        "전처리 API를 사용하면 전처리를 모델의 일부로 만들어 애플리케이션 코드와 추가 이미지 처리 라이브러리에 대한 종속성을 줄일 수 있습니다.\n",
        "전처리 API의 주요 장점은 전처리 단계가 실행 그래프에 통합되어 애플리케이션의 일부로 CPU에서 항상 실행되는 대신 선택한 장치(CPU/GPU 등)에서 수행된다는 것입니다. 이렇게 하면 선택한 장치 활용도가 향상됩니다.\n",
        "\n",
        "자세한 내용은 [전처리 API](https://docs.openvino.ai/2023.0/openvino_docs_OV_Runtime_UG_Preprocessing_Overview.html) 개요를 참조하세요.\n",
        "\n",
        "예를 들어 'image_to_tensor' 함수에 정의된 입력 데이터 레이아웃 변환과 정규화를 통합할 수 있습니다.\n",
        "\n",
        "통합 프로세스는 다음 단계로 구성됩니다.\n",
        "1. PrePostProcessing 객체를 초기화합니다.\n",
        "2. 입력 데이터 형식을 정의합니다.\n",
        "3. 전처리 단계를 설명하세요.\n",
        "4. 모델에 단계 통합."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42a07a18-74ef-4802-89d5-2ae99acbf515",
      "metadata": {
        "id": "42a07a18-74ef-4802-89d5-2ae99acbf515"
      },
      "source": [
        "#### PrePostProcessing API 초기화 [$\\Uparrow$](#목차:)\n",
        "\n",
        "'openvino.preprocess.PrePostProcessor' 클래스를 사용하면 모델의 전처리 및 후처리 단계를 지정할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6582080b-d5fb-4662-a8b8-c4a14c9998eb",
      "metadata": {
        "id": "6582080b-d5fb-4662-a8b8-c4a14c9998eb"
      },
      "outputs": [],
      "source": [
        "from openvino.preprocess import PrePostProcessor\n",
        "\n",
        "ppp = PrePostProcessor(quantized_det_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f249e086-eb5b-4caa-aeca-9fa6d5326c37",
      "metadata": {
        "id": "f249e086-eb5b-4caa-aeca-9fa6d5326c37"
      },
      "source": [
        "#### 입력 데이터 형식 정의 [$\\Uparrow$](#목차:)\n",
        "모델/전처리기의 특정 입력을 처리하기 위해 `input(input_id)` 메서드. 여기서 `input_id`는 `model.inputs` 입력에 대한 위치 인덱스 또는 입력 텐서 이름입니다. 모델에 단일 입력 `input_id가 있는 경우 `는 생략 가능합니다.\n",
        "디스크에서 이미지를 읽은 후 '[0, 255]' 범위의 U8 픽셀을 포함하고 'NHWC' 레이아웃에 저장됩니다. 전처리 변환을 수행하려면 이를 텐서 설명에 제공해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e200bcb0-aea1-4229-ae5c-fe0fc45989a8",
      "metadata": {
        "id": "e200bcb0-aea1-4229-ae5c-fe0fc45989a8"
      },
      "outputs": [],
      "source": [
        "ppp.input(0).tensor().set_shape([1, 640, 640, 3]).set_element_type(ov.Type.u8).set_layout(ov.Layout('NHWC'))\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5a0233-45c9-496b-945b-4e563c5066ff",
      "metadata": {
        "id": "9c5a0233-45c9-496b-945b-4e563c5066ff"
      },
      "source": [
        "레이아웃 변환을 수행하려면 모델에서 예상하는 레이아웃에 대한 정보도 제공해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb1e999-6798-4de9-9ec5-5ded4f77d1ac",
      "metadata": {
        "id": "3fb1e999-6798-4de9-9ec5-5ded4f77d1ac"
      },
      "source": [
        "#### 전처리 단계 설명 [$\\Uparrow$](#목차:)\n",
        "\n",
        "전처리 기능에는 다음 단계가 포함됩니다.\n",
        "* 데이터 유형을 'U8'에서 'FP32'로 변환합니다.\n",
        "* 데이터 레이아웃을 'NHWC'에서 'NCHW' 형식으로 변환합니다.\n",
        "* 배율 255로 나누어 각 픽셀을 정규화합니다.\n",
        "\n",
        "`ppp.input(input_id).preprocess()`는 일련의 전처리 단계를 정의하는 데 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191b4885-ecba-43e0-b88d-dc2fbaa342f4",
      "metadata": {
        "id": "191b4885-ecba-43e0-b88d-dc2fbaa342f4"
      },
      "outputs": [],
      "source": [
        "ppp.input(0).preprocess().convert_element_type(ov.Type.f32).convert_layout(ov.Layout('NCHW')).scale([255., 255., 255.])\n",
        "\n",
        "print(ppp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5ab2ca-3082-4d63-914d-4c1c4e5b9a6b",
      "metadata": {
        "id": "8a5ab2ca-3082-4d63-914d-4c1c4e5b9a6b"
      },
      "source": [
        "#### 모델에 단계 통합 [$\\Uparrow$](#목차:)\n",
        "\n",
        "전처리 단계가 완료되면 최종적으로 모델을 구축할 수 있습니다. 또한 `openvino.runtime.serialize`를 사용하여 완성된 모델을 OpenVINO IR에 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63352458-b798-4880-8e57-e291483499f1",
      "metadata": {
        "id": "63352458-b798-4880-8e57-e291483499f1"
      },
      "outputs": [],
      "source": [
        "quantized_model_with_preprocess = ppp.build()\n",
        "serialize(quantized_model_with_preprocess, str(int8_model_det_path.with_name(f\"{DET_MODEL_NAME}_with_preprocess.xml\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636f449e-36a9-42b3-af5b-4995036ba391",
      "metadata": {
        "id": "636f449e-36a9-42b3-af5b-4995036ba391"
      },
      "source": [
        "전처리가 통합된 모델을 기기에 로드할 준비가 되었습니다. 이제 감지 기능에서 다음 전처리 단계를 건너뛸 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8177583e-510f-4e43-be26-253e5d27621e",
      "metadata": {
        "id": "8177583e-510f-4e43-be26-253e5d27621e"
      },
      "outputs": [],
      "source": [
        "def detect_without_preprocess(image:np.ndarray, model:ov.Model):\n",
        "    \"\"\"\n",
        "    OpenVINO YOLOv8 model with integrated preprocessing inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
        "    Parameters:\n",
        "        image (np.ndarray): input image.\n",
        "        model (Model): OpenVINO compiled model.\n",
        "    Returns:\n",
        "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
        "    \"\"\"\n",
        "    output_layer = model.output(0)\n",
        "    img = letterbox(image)[0]\n",
        "    input_tensor = np.expand_dims(img, 0)\n",
        "    input_hw = img.shape[:2]\n",
        "    result = model(input_tensor)[output_layer]\n",
        "    detections = postprocess(result, input_hw, image)\n",
        "    return detections\n",
        "\n",
        "\n",
        "compiled_model = core.compile_model(quantized_model_with_preprocess, device.value)\n",
        "input_image = np.array(Image.open(IMAGE_PATH))\n",
        "detections = detect_without_preprocess(input_image, compiled_model)[0]\n",
        "image_with_boxes = draw_results(detections, input_image, label_map)\n",
        "\n",
        "Image.fromarray(image_with_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
      "metadata": {
        "tags": [],
        "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8"
      },
      "source": [
        "## 라이브 데모 [$\\Uparrow$](#목차:)\n",
        "\n",
        "다음 코드는 비디오에서 모델 추론을 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
      "metadata": {
        "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "# Main processing function to run object detection.\n",
        "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0, model=det_model, device=device.value):\n",
        "    player = None\n",
        "    if device != \"CPU\":\n",
        "        model.reshape({0: [1, 3, 640, 640]})\n",
        "    compiled_model = core.compile_model(model, device)\n",
        "    try:\n",
        "        # Create a video player to play with target fps.\n",
        "        player = VideoPlayer(\n",
        "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
        "        )\n",
        "        # Start capturing.\n",
        "        player.start()\n",
        "        if use_popup:\n",
        "            title = \"Press ESC to Exit\"\n",
        "            cv2.namedWindow(\n",
        "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
        "            )\n",
        "\n",
        "        processing_times = collections.deque()\n",
        "        while True:\n",
        "            # Grab the frame.\n",
        "            frame = player.next()\n",
        "            if frame is None:\n",
        "                print(\"Source ended\")\n",
        "                break\n",
        "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
        "            scale = 1280 / max(frame.shape)\n",
        "            if scale < 1:\n",
        "                frame = cv2.resize(\n",
        "                    src=frame,\n",
        "                    dsize=None,\n",
        "                    fx=scale,\n",
        "                    fy=scale,\n",
        "                    interpolation=cv2.INTER_AREA,\n",
        "                )\n",
        "            # Get the results.\n",
        "            input_image = np.array(frame)\n",
        "\n",
        "            start_time = time.time()\n",
        "            # model expects RGB image, while video capturing in BGR\n",
        "            detections = detect(input_image[:, :, ::-1], compiled_model)[0]\n",
        "            stop_time = time.time()\n",
        "\n",
        "            image_with_boxes = draw_results(detections, input_image, label_map)\n",
        "            frame = image_with_boxes\n",
        "\n",
        "            processing_times.append(stop_time - start_time)\n",
        "            # Use processing times from last 200 frames.\n",
        "            if len(processing_times) > 200:\n",
        "                processing_times.popleft()\n",
        "\n",
        "            _, f_width = frame.shape[:2]\n",
        "            # Mean processing time [ms].\n",
        "            processing_time = np.mean(processing_times) * 1000\n",
        "            fps = 1000 / processing_time\n",
        "            cv2.putText(\n",
        "                img=frame,\n",
        "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
        "                org=(20, 40),\n",
        "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
        "                fontScale=f_width / 1000,\n",
        "                color=(0, 0, 255),\n",
        "                thickness=1,\n",
        "                lineType=cv2.LINE_AA,\n",
        "            )\n",
        "            # Use this workaround if there is flickering.\n",
        "            if use_popup:\n",
        "                cv2.imshow(winname=title, mat=frame)\n",
        "                key = cv2.waitKey(1)\n",
        "                # escape = 27\n",
        "                if key == 27:\n",
        "                    break\n",
        "            else:\n",
        "                # Encode numpy array to jpg.\n",
        "                _, encoded_img = cv2.imencode(\n",
        "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
        "                )\n",
        "                # Create an IPython image.\n",
        "                i = display.Image(data=encoded_img)\n",
        "                # Display the image in this notebook.\n",
        "                display.clear_output(wait=True)\n",
        "                display.display(i)\n",
        "    # ctrl-c\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    # any different error\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "    finally:\n",
        "        if player is not None:\n",
        "            # Stop capturing.\n",
        "            player.stop()\n",
        "        if use_popup:\n",
        "            cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
      "metadata": {
        "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41"
      },
      "source": [
        "### 실시간 개체 감지 실행 [$\\Uparrow$](#목차:)\n",
        "\n",
        "웹캠을 비디오 입력으로 사용하십시오. 기본적으로 기본 웹캠은 `source=0`으로 설정됩니다. 웹캠이 여러 개인 경우 각 웹캠에는 0부터 시작하는 연속 번호가 할당됩니다. 전면 카메라를 사용할 때는 `flip=True`를 설정하세요. 일부 웹 브라우저, 특히 Mozilla Firefox에서는 깜박임이 발생할 수 있습니다. 깜박임이 발생하면 `use_popup=True`를 설정하세요.\n",
        "\n",
        ">**참고**: 이 노트북을 웹캠과 함께 사용하려면 웹캠이 있는 컴퓨터에서 노트북을 실행해야 합니다. 원격 서버(예: Binder 또는 Google Colab 서비스)에서 노트북을 실행하면 웹캠이 작동하지 않습니다. 기본적으로 아래쪽 셀은 비디오 파일에 대한 모델 추론을 실행합니다. 웹캠에서 실시간 추론을 시도하려면 'WEBCAM_INFERENCE = True'로 설정하세요.\n",
        "\n",
        "객체 감지를 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90708017",
      "metadata": {
        "id": "90708017"
      },
      "outputs": [],
      "source": [
        "WEBCAM_INFERENCE = False\n",
        "\n",
        "if WEBCAM_INFERENCE:\n",
        "    VIDEO_SOURCE = 0  # Webcam\n",
        "else:\n",
        "    VIDEO_SOURCE = 'https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/people.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee30d986",
      "metadata": {
        "id": "ee30d986"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880",
      "metadata": {
        "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880"
      },
      "outputs": [],
      "source": [
        "run_object_detection(source=VIDEO_SOURCE, flip=True, use_popup=False, model=det_ov_model, device=device.value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}